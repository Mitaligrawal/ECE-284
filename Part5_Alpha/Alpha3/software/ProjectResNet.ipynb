{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "Final_ResNet_Cifar(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): SqueezeBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.125)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 8, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): SqueezeBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.125)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): SqueezeBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.125)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          8, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): SqueezeBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.125)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          64, 8, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): SqueezeBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.125)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): SqueezeBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.125)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=4, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=8, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../../software')\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"final_ResNet20_quant\"\n",
    "model = final_resnet20_quant()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d354a1-9d41-42cc-9ff6-5152d325891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 1.043 (1.043)\tData 0.558 (0.558)\tLoss 2.4538 (2.4538)\tPrec 10.156% (10.156%)\n",
      "Epoch: [0][100/391]\tTime 0.044 (0.068)\tData 0.002 (0.010)\tLoss 2.2594 (2.3116)\tPrec 17.188% (12.129%)\n",
      "Epoch: [0][200/391]\tTime 0.048 (0.058)\tData 0.002 (0.006)\tLoss 2.1101 (2.2363)\tPrec 13.281% (15.388%)\n",
      "Epoch: [0][300/391]\tTime 0.047 (0.055)\tData 0.002 (0.005)\tLoss 1.9016 (2.1629)\tPrec 25.000% (17.566%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.481 (0.481)\tLoss 2.0057 (2.0057)\tPrec 28.906% (28.906%)\n",
      " * Prec 24.220% \n",
      "best acc: 24.220000\n",
      "Epoch: [1][0/391]\tTime 0.801 (0.801)\tData 0.729 (0.729)\tLoss 1.9395 (1.9395)\tPrec 28.125% (28.125%)\n",
      "Epoch: [1][100/391]\tTime 0.053 (0.059)\tData 0.002 (0.009)\tLoss 1.7645 (1.8773)\tPrec 25.000% (25.696%)\n",
      "Epoch: [1][200/391]\tTime 0.054 (0.054)\tData 0.003 (0.006)\tLoss 1.9080 (1.8592)\tPrec 27.344% (26.632%)\n",
      "Epoch: [1][300/391]\tTime 0.054 (0.053)\tData 0.002 (0.005)\tLoss 1.7276 (1.8330)\tPrec 35.938% (27.842%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.597 (0.597)\tLoss 1.6736 (1.6736)\tPrec 32.031% (32.031%)\n",
      " * Prec 33.890% \n",
      "best acc: 33.890000\n",
      "Epoch: [2][0/391]\tTime 0.626 (0.626)\tData 0.554 (0.554)\tLoss 1.7795 (1.7795)\tPrec 37.500% (37.500%)\n",
      "Epoch: [2][100/391]\tTime 0.045 (0.059)\tData 0.002 (0.008)\tLoss 1.8231 (1.7307)\tPrec 32.031% (32.464%)\n",
      "Epoch: [2][200/391]\tTime 0.054 (0.056)\tData 0.002 (0.005)\tLoss 1.6523 (1.7218)\tPrec 32.812% (33.104%)\n",
      "Epoch: [2][300/391]\tTime 0.061 (0.055)\tData 0.003 (0.004)\tLoss 1.7497 (1.7091)\tPrec 32.812% (33.503%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.506 (0.506)\tLoss 1.6132 (1.6132)\tPrec 36.719% (36.719%)\n",
      " * Prec 37.900% \n",
      "best acc: 37.900000\n",
      "Epoch: [3][0/391]\tTime 0.780 (0.780)\tData 0.709 (0.709)\tLoss 1.6787 (1.6787)\tPrec 28.906% (28.906%)\n",
      "Epoch: [3][100/391]\tTime 0.059 (0.061)\tData 0.002 (0.009)\tLoss 1.5752 (1.6425)\tPrec 37.500% (36.595%)\n",
      "Epoch: [3][200/391]\tTime 0.060 (0.059)\tData 0.003 (0.006)\tLoss 1.6460 (1.6329)\tPrec 39.844% (37.228%)\n",
      "Epoch: [3][300/391]\tTime 0.061 (0.059)\tData 0.003 (0.005)\tLoss 1.6650 (1.6224)\tPrec 40.625% (37.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.480 (0.480)\tLoss 1.5180 (1.5180)\tPrec 42.969% (42.969%)\n",
      " * Prec 41.880% \n",
      "best acc: 41.880000\n",
      "Epoch: [4][0/391]\tTime 0.756 (0.756)\tData 0.687 (0.687)\tLoss 1.7273 (1.7273)\tPrec 37.500% (37.500%)\n",
      "Epoch: [4][100/391]\tTime 0.056 (0.057)\tData 0.002 (0.009)\tLoss 1.6080 (1.5631)\tPrec 41.406% (40.857%)\n",
      "Epoch: [4][200/391]\tTime 0.055 (0.053)\tData 0.003 (0.006)\tLoss 1.6786 (1.5453)\tPrec 37.500% (41.663%)\n",
      "Epoch: [4][300/391]\tTime 0.045 (0.052)\tData 0.002 (0.004)\tLoss 1.4778 (1.5372)\tPrec 39.844% (41.827%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.276 (0.276)\tLoss 1.5278 (1.5278)\tPrec 40.625% (40.625%)\n",
      " * Prec 45.540% \n",
      "best acc: 45.540000\n",
      "Epoch: [5][0/391]\tTime 0.636 (0.636)\tData 0.562 (0.562)\tLoss 1.5548 (1.5548)\tPrec 38.281% (38.281%)\n",
      "Epoch: [5][100/391]\tTime 0.055 (0.058)\tData 0.002 (0.008)\tLoss 1.4827 (1.4650)\tPrec 42.188% (45.111%)\n",
      "Epoch: [5][200/391]\tTime 0.060 (0.055)\tData 0.002 (0.005)\tLoss 1.3861 (1.4653)\tPrec 46.094% (45.382%)\n",
      "Epoch: [5][300/391]\tTime 0.044 (0.055)\tData 0.002 (0.004)\tLoss 1.4572 (1.4586)\tPrec 49.219% (45.619%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.600 (0.600)\tLoss 1.5297 (1.5297)\tPrec 39.844% (39.844%)\n",
      " * Prec 45.720% \n",
      "best acc: 45.720000\n",
      "Epoch: [6][0/391]\tTime 0.801 (0.801)\tData 0.734 (0.734)\tLoss 1.4897 (1.4897)\tPrec 45.312% (45.312%)\n",
      "Epoch: [6][100/391]\tTime 0.047 (0.057)\tData 0.003 (0.009)\tLoss 1.2442 (1.4091)\tPrec 50.000% (47.765%)\n",
      "Epoch: [6][200/391]\tTime 0.054 (0.053)\tData 0.003 (0.006)\tLoss 1.4633 (1.3997)\tPrec 44.531% (48.064%)\n",
      "Epoch: [6][300/391]\tTime 0.049 (0.051)\tData 0.002 (0.005)\tLoss 1.3467 (1.3893)\tPrec 54.688% (48.585%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.334 (0.334)\tLoss 1.3928 (1.3928)\tPrec 53.906% (53.906%)\n",
      " * Prec 50.990% \n",
      "best acc: 50.990000\n",
      "Epoch: [7][0/391]\tTime 0.842 (0.842)\tData 0.775 (0.775)\tLoss 1.3676 (1.3676)\tPrec 46.094% (46.094%)\n",
      "Epoch: [7][100/391]\tTime 0.044 (0.067)\tData 0.002 (0.010)\tLoss 1.4177 (1.3403)\tPrec 43.750% (50.905%)\n",
      "Epoch: [7][200/391]\tTime 0.054 (0.063)\tData 0.002 (0.006)\tLoss 1.2479 (1.3393)\tPrec 56.250% (50.816%)\n",
      "Epoch: [7][300/391]\tTime 0.047 (0.058)\tData 0.002 (0.005)\tLoss 1.5249 (1.3351)\tPrec 43.750% (51.142%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.391 (0.391)\tLoss 1.2477 (1.2477)\tPrec 53.906% (53.906%)\n",
      " * Prec 53.500% \n",
      "best acc: 53.500000\n",
      "Epoch: [8][0/391]\tTime 0.594 (0.594)\tData 0.522 (0.522)\tLoss 1.2922 (1.2922)\tPrec 49.219% (49.219%)\n",
      "Epoch: [8][100/391]\tTime 0.043 (0.054)\tData 0.002 (0.007)\tLoss 1.2479 (1.2929)\tPrec 55.469% (52.653%)\n",
      "Epoch: [8][200/391]\tTime 0.041 (0.051)\tData 0.002 (0.005)\tLoss 1.2829 (1.2836)\tPrec 53.906% (53.102%)\n",
      "Epoch: [8][300/391]\tTime 0.044 (0.050)\tData 0.002 (0.004)\tLoss 1.1433 (1.2752)\tPrec 53.906% (53.551%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.521 (0.521)\tLoss 1.2813 (1.2813)\tPrec 56.250% (56.250%)\n",
      " * Prec 53.080% \n",
      "best acc: 53.500000\n",
      "Epoch: [9][0/391]\tTime 0.625 (0.625)\tData 0.557 (0.557)\tLoss 1.2919 (1.2919)\tPrec 58.594% (58.594%)\n",
      "Epoch: [9][100/391]\tTime 0.061 (0.065)\tData 0.003 (0.008)\tLoss 1.3412 (1.2443)\tPrec 47.656% (55.105%)\n",
      "Epoch: [9][200/391]\tTime 0.058 (0.062)\tData 0.002 (0.005)\tLoss 1.1344 (1.2281)\tPrec 57.812% (55.519%)\n",
      "Epoch: [9][300/391]\tTime 0.061 (0.061)\tData 0.002 (0.004)\tLoss 1.3054 (1.2208)\tPrec 54.688% (55.952%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.327 (0.327)\tLoss 1.2337 (1.2337)\tPrec 54.688% (54.688%)\n",
      " * Prec 54.970% \n",
      "best acc: 54.970000\n",
      "Epoch: [10][0/391]\tTime 0.654 (0.654)\tData 0.586 (0.586)\tLoss 1.1397 (1.1397)\tPrec 57.031% (57.031%)\n",
      "Epoch: [10][100/391]\tTime 0.050 (0.055)\tData 0.002 (0.008)\tLoss 1.2381 (1.1788)\tPrec 55.469% (57.395%)\n",
      "Epoch: [10][200/391]\tTime 0.051 (0.052)\tData 0.002 (0.005)\tLoss 1.1336 (1.1837)\tPrec 55.469% (57.315%)\n",
      "Epoch: [10][300/391]\tTime 0.044 (0.050)\tData 0.002 (0.004)\tLoss 1.2774 (1.1832)\tPrec 58.594% (57.229%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.516 (0.516)\tLoss 1.1553 (1.1553)\tPrec 58.594% (58.594%)\n",
      " * Prec 56.850% \n",
      "best acc: 56.850000\n",
      "Epoch: [11][0/391]\tTime 0.574 (0.574)\tData 0.507 (0.507)\tLoss 1.0819 (1.0819)\tPrec 62.500% (62.500%)\n",
      "Epoch: [11][100/391]\tTime 0.046 (0.058)\tData 0.002 (0.007)\tLoss 1.0920 (1.1307)\tPrec 60.156% (59.352%)\n",
      "Epoch: [11][200/391]\tTime 0.054 (0.056)\tData 0.003 (0.005)\tLoss 0.9919 (1.1303)\tPrec 64.844% (59.266%)\n",
      "Epoch: [11][300/391]\tTime 0.047 (0.055)\tData 0.002 (0.004)\tLoss 1.0770 (1.1262)\tPrec 54.688% (59.224%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.431 (0.431)\tLoss 0.9770 (0.9770)\tPrec 65.625% (65.625%)\n",
      " * Prec 60.050% \n",
      "best acc: 60.050000\n",
      "Epoch: [12][0/391]\tTime 0.709 (0.709)\tData 0.644 (0.644)\tLoss 1.2171 (1.2171)\tPrec 52.344% (52.344%)\n",
      "Epoch: [12][100/391]\tTime 0.048 (0.057)\tData 0.003 (0.009)\tLoss 0.9873 (1.0964)\tPrec 61.719% (60.334%)\n",
      "Epoch: [12][200/391]\tTime 0.058 (0.054)\tData 0.002 (0.005)\tLoss 1.1829 (1.0905)\tPrec 57.031% (60.518%)\n",
      "Epoch: [12][300/391]\tTime 0.048 (0.052)\tData 0.002 (0.004)\tLoss 0.9892 (1.0845)\tPrec 64.062% (60.795%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.518 (0.518)\tLoss 0.9964 (0.9964)\tPrec 64.062% (64.062%)\n",
      " * Prec 59.260% \n",
      "best acc: 60.050000\n",
      "Epoch: [13][0/391]\tTime 0.514 (0.514)\tData 0.446 (0.446)\tLoss 1.0070 (1.0070)\tPrec 59.375% (59.375%)\n",
      "Epoch: [13][100/391]\tTime 0.057 (0.057)\tData 0.002 (0.007)\tLoss 1.0215 (1.0588)\tPrec 57.812% (61.750%)\n",
      "Epoch: [13][200/391]\tTime 0.046 (0.055)\tData 0.002 (0.004)\tLoss 1.0773 (1.0561)\tPrec 59.375% (62.069%)\n",
      "Epoch: [13][300/391]\tTime 0.047 (0.053)\tData 0.002 (0.004)\tLoss 1.1309 (1.0479)\tPrec 60.938% (62.207%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.457 (0.457)\tLoss 0.9630 (0.9630)\tPrec 62.500% (62.500%)\n",
      " * Prec 63.030% \n",
      "best acc: 63.030000\n",
      "Epoch: [14][0/391]\tTime 0.853 (0.853)\tData 0.782 (0.782)\tLoss 1.0220 (1.0220)\tPrec 60.156% (60.156%)\n",
      "Epoch: [14][100/391]\tTime 0.046 (0.060)\tData 0.002 (0.010)\tLoss 0.9193 (1.0149)\tPrec 64.844% (63.335%)\n",
      "Epoch: [14][200/391]\tTime 0.054 (0.056)\tData 0.003 (0.006)\tLoss 1.0912 (0.9975)\tPrec 64.844% (64.342%)\n",
      "Epoch: [14][300/391]\tTime 0.045 (0.054)\tData 0.002 (0.005)\tLoss 0.8921 (1.0027)\tPrec 67.969% (64.146%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.342 (0.342)\tLoss 0.8818 (0.8818)\tPrec 67.969% (67.969%)\n",
      " * Prec 62.030% \n",
      "best acc: 63.030000\n",
      "Epoch: [15][0/391]\tTime 0.749 (0.749)\tData 0.678 (0.678)\tLoss 1.1089 (1.1089)\tPrec 62.500% (62.500%)\n",
      "Epoch: [15][100/391]\tTime 0.058 (0.063)\tData 0.002 (0.009)\tLoss 0.9597 (1.0073)\tPrec 67.188% (63.691%)\n",
      "Epoch: [15][200/391]\tTime 0.043 (0.057)\tData 0.002 (0.006)\tLoss 0.8821 (0.9917)\tPrec 64.062% (64.226%)\n",
      "Epoch: [15][300/391]\tTime 0.048 (0.054)\tData 0.002 (0.004)\tLoss 0.9943 (0.9802)\tPrec 64.844% (64.857%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.585 (0.585)\tLoss 0.8254 (0.8254)\tPrec 66.406% (66.406%)\n",
      " * Prec 64.740% \n",
      "best acc: 64.740000\n",
      "Epoch: [16][0/391]\tTime 0.566 (0.566)\tData 0.496 (0.496)\tLoss 1.0350 (1.0350)\tPrec 62.500% (62.500%)\n",
      "Epoch: [16][100/391]\tTime 0.050 (0.053)\tData 0.002 (0.007)\tLoss 0.9082 (0.9649)\tPrec 67.188% (65.099%)\n",
      "Epoch: [16][200/391]\tTime 0.049 (0.051)\tData 0.002 (0.005)\tLoss 0.8704 (0.9571)\tPrec 63.281% (65.322%)\n",
      "Epoch: [16][300/391]\tTime 0.053 (0.050)\tData 0.002 (0.004)\tLoss 0.7250 (0.9506)\tPrec 76.562% (65.685%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.440 (0.440)\tLoss 0.8822 (0.8822)\tPrec 65.625% (65.625%)\n",
      " * Prec 64.970% \n",
      "best acc: 64.970000\n",
      "Epoch: [17][0/391]\tTime 0.734 (0.734)\tData 0.670 (0.670)\tLoss 0.9061 (0.9061)\tPrec 65.625% (65.625%)\n",
      "Epoch: [17][100/391]\tTime 0.049 (0.057)\tData 0.002 (0.009)\tLoss 0.8182 (0.9323)\tPrec 73.438% (66.569%)\n",
      "Epoch: [17][200/391]\tTime 0.058 (0.053)\tData 0.002 (0.005)\tLoss 0.8921 (0.9288)\tPrec 72.656% (66.573%)\n",
      "Epoch: [17][300/391]\tTime 0.048 (0.051)\tData 0.002 (0.004)\tLoss 0.8416 (0.9239)\tPrec 77.344% (66.715%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.537 (0.537)\tLoss 0.9551 (0.9551)\tPrec 60.938% (60.938%)\n",
      " * Prec 65.240% \n",
      "best acc: 65.240000\n",
      "Epoch: [18][0/391]\tTime 0.622 (0.622)\tData 0.553 (0.553)\tLoss 0.8682 (0.8682)\tPrec 65.625% (65.625%)\n",
      "Epoch: [18][100/391]\tTime 0.044 (0.056)\tData 0.002 (0.008)\tLoss 0.9477 (0.9311)\tPrec 65.625% (66.538%)\n",
      "Epoch: [18][200/391]\tTime 0.047 (0.053)\tData 0.002 (0.005)\tLoss 0.8723 (0.9155)\tPrec 68.750% (66.966%)\n",
      "Epoch: [18][300/391]\tTime 0.054 (0.052)\tData 0.003 (0.004)\tLoss 0.8140 (0.9095)\tPrec 72.656% (67.382%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.520 (0.520)\tLoss 0.8856 (0.8856)\tPrec 65.625% (65.625%)\n",
      " * Prec 67.200% \n",
      "best acc: 67.200000\n",
      "Epoch: [19][0/391]\tTime 0.651 (0.651)\tData 0.581 (0.581)\tLoss 0.8708 (0.8708)\tPrec 68.750% (68.750%)\n",
      "Epoch: [19][100/391]\tTime 0.054 (0.059)\tData 0.003 (0.008)\tLoss 0.7844 (0.8731)\tPrec 71.875% (68.270%)\n",
      "Epoch: [19][200/391]\tTime 0.048 (0.056)\tData 0.002 (0.005)\tLoss 0.7862 (0.8798)\tPrec 71.875% (68.066%)\n",
      "Epoch: [19][300/391]\tTime 0.048 (0.054)\tData 0.002 (0.004)\tLoss 0.8513 (0.8756)\tPrec 69.531% (68.514%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.440 (0.440)\tLoss 0.8614 (0.8614)\tPrec 67.969% (67.969%)\n",
      " * Prec 66.240% \n",
      "best acc: 67.200000\n",
      "Epoch: [20][0/391]\tTime 0.636 (0.636)\tData 0.573 (0.573)\tLoss 0.5886 (0.5886)\tPrec 78.125% (78.125%)\n",
      "Epoch: [20][100/391]\tTime 0.046 (0.055)\tData 0.002 (0.008)\tLoss 0.8341 (0.8681)\tPrec 70.312% (69.438%)\n",
      "Epoch: [20][200/391]\tTime 0.047 (0.052)\tData 0.002 (0.005)\tLoss 1.0043 (0.8585)\tPrec 67.188% (69.523%)\n",
      "Epoch: [20][300/391]\tTime 0.040 (0.051)\tData 0.002 (0.004)\tLoss 0.6778 (0.8508)\tPrec 74.219% (69.773%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.422 (0.422)\tLoss 0.9425 (0.9425)\tPrec 63.281% (63.281%)\n",
      " * Prec 66.980% \n",
      "best acc: 67.200000\n",
      "Epoch: [21][0/391]\tTime 0.642 (0.642)\tData 0.588 (0.588)\tLoss 0.7401 (0.7401)\tPrec 73.438% (73.438%)\n",
      "Epoch: [21][100/391]\tTime 0.051 (0.054)\tData 0.002 (0.008)\tLoss 0.8766 (0.8343)\tPrec 71.094% (70.135%)\n",
      "Epoch: [21][200/391]\tTime 0.055 (0.051)\tData 0.002 (0.005)\tLoss 0.6822 (0.8251)\tPrec 75.000% (70.367%)\n",
      "Epoch: [21][300/391]\tTime 0.053 (0.050)\tData 0.003 (0.004)\tLoss 0.8177 (0.8314)\tPrec 67.969% (70.284%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.411 (0.411)\tLoss 0.8747 (0.8747)\tPrec 64.844% (64.844%)\n",
      " * Prec 68.170% \n",
      "best acc: 68.170000\n",
      "Epoch: [22][0/391]\tTime 0.634 (0.634)\tData 0.568 (0.568)\tLoss 0.8993 (0.8993)\tPrec 68.750% (68.750%)\n",
      "Epoch: [22][100/391]\tTime 0.043 (0.055)\tData 0.002 (0.008)\tLoss 0.8242 (0.8310)\tPrec 66.406% (70.336%)\n",
      "Epoch: [22][200/391]\tTime 0.047 (0.050)\tData 0.002 (0.005)\tLoss 0.8868 (0.8206)\tPrec 68.750% (70.857%)\n",
      "Epoch: [22][300/391]\tTime 0.043 (0.049)\tData 0.002 (0.004)\tLoss 0.8077 (0.8111)\tPrec 67.188% (71.234%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.431 (0.431)\tLoss 0.7512 (0.7512)\tPrec 75.000% (75.000%)\n",
      " * Prec 71.090% \n",
      "best acc: 71.090000\n",
      "Epoch: [23][0/391]\tTime 0.829 (0.829)\tData 0.756 (0.756)\tLoss 0.7907 (0.7907)\tPrec 72.656% (72.656%)\n",
      "Epoch: [23][100/391]\tTime 0.043 (0.056)\tData 0.002 (0.010)\tLoss 0.9462 (0.7832)\tPrec 61.719% (72.331%)\n",
      "Epoch: [23][200/391]\tTime 0.051 (0.053)\tData 0.002 (0.006)\tLoss 0.7469 (0.7842)\tPrec 75.781% (72.198%)\n",
      "Epoch: [23][300/391]\tTime 0.046 (0.053)\tData 0.002 (0.005)\tLoss 0.7149 (0.7851)\tPrec 78.125% (72.153%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.264 (0.264)\tLoss 0.9422 (0.9422)\tPrec 68.750% (68.750%)\n",
      " * Prec 69.450% \n",
      "best acc: 71.090000\n",
      "Epoch: [24][0/391]\tTime 0.753 (0.753)\tData 0.687 (0.687)\tLoss 0.8568 (0.8568)\tPrec 74.219% (74.219%)\n",
      "Epoch: [24][100/391]\tTime 0.047 (0.056)\tData 0.002 (0.009)\tLoss 0.7974 (0.7621)\tPrec 68.750% (72.803%)\n",
      "Epoch: [24][200/391]\tTime 0.043 (0.052)\tData 0.002 (0.006)\tLoss 0.8458 (0.7656)\tPrec 68.750% (72.839%)\n",
      "Epoch: [24][300/391]\tTime 0.055 (0.051)\tData 0.002 (0.004)\tLoss 1.0837 (0.7682)\tPrec 64.062% (72.768%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.284 (0.284)\tLoss 0.9261 (0.9261)\tPrec 64.844% (64.844%)\n",
      " * Prec 69.030% \n",
      "best acc: 71.090000\n",
      "Epoch: [25][0/391]\tTime 0.572 (0.572)\tData 0.493 (0.493)\tLoss 0.8304 (0.8304)\tPrec 74.219% (74.219%)\n",
      "Epoch: [25][100/391]\tTime 0.047 (0.055)\tData 0.002 (0.007)\tLoss 0.6897 (0.7509)\tPrec 75.781% (73.623%)\n",
      "Epoch: [25][200/391]\tTime 0.045 (0.051)\tData 0.001 (0.005)\tLoss 0.8646 (0.7495)\tPrec 69.531% (73.511%)\n",
      "Epoch: [25][300/391]\tTime 0.043 (0.050)\tData 0.002 (0.004)\tLoss 0.6753 (0.7498)\tPrec 74.219% (73.463%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.460 (0.460)\tLoss 0.8708 (0.8708)\tPrec 69.531% (69.531%)\n",
      " * Prec 71.010% \n",
      "best acc: 71.090000\n",
      "Epoch: [26][0/391]\tTime 0.675 (0.675)\tData 0.609 (0.609)\tLoss 0.8049 (0.8049)\tPrec 76.562% (76.562%)\n",
      "Epoch: [26][100/391]\tTime 0.048 (0.054)\tData 0.002 (0.008)\tLoss 0.6527 (0.7413)\tPrec 76.562% (73.461%)\n",
      "Epoch: [26][200/391]\tTime 0.048 (0.051)\tData 0.002 (0.005)\tLoss 0.7204 (0.7393)\tPrec 74.219% (73.686%)\n",
      "Epoch: [26][300/391]\tTime 0.052 (0.050)\tData 0.002 (0.004)\tLoss 0.7752 (0.7334)\tPrec 74.219% (73.959%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.396 (0.396)\tLoss 0.8443 (0.8443)\tPrec 71.875% (71.875%)\n",
      " * Prec 72.320% \n",
      "best acc: 72.320000\n",
      "Epoch: [27][0/391]\tTime 0.686 (0.686)\tData 0.617 (0.617)\tLoss 0.7721 (0.7721)\tPrec 75.000% (75.000%)\n",
      "Epoch: [27][100/391]\tTime 0.044 (0.055)\tData 0.002 (0.008)\tLoss 0.6346 (0.7088)\tPrec 80.469% (75.015%)\n",
      "Epoch: [27][200/391]\tTime 0.046 (0.052)\tData 0.002 (0.005)\tLoss 0.7836 (0.7109)\tPrec 71.875% (75.109%)\n",
      "Epoch: [27][300/391]\tTime 0.054 (0.053)\tData 0.002 (0.004)\tLoss 0.7993 (0.7067)\tPrec 67.188% (75.125%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.409 (0.409)\tLoss 0.7976 (0.7976)\tPrec 71.094% (71.094%)\n",
      " * Prec 72.820% \n",
      "best acc: 72.820000\n",
      "Epoch: [28][0/391]\tTime 0.627 (0.627)\tData 0.566 (0.566)\tLoss 0.6117 (0.6117)\tPrec 75.000% (75.000%)\n",
      "Epoch: [28][100/391]\tTime 0.047 (0.055)\tData 0.002 (0.008)\tLoss 0.6485 (0.6870)\tPrec 78.125% (75.704%)\n",
      "Epoch: [28][200/391]\tTime 0.051 (0.052)\tData 0.002 (0.005)\tLoss 0.5621 (0.6848)\tPrec 79.688% (75.941%)\n",
      "Epoch: [28][300/391]\tTime 0.045 (0.051)\tData 0.002 (0.004)\tLoss 0.5579 (0.6889)\tPrec 80.469% (75.877%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.492 (0.492)\tLoss 0.8843 (0.8843)\tPrec 70.312% (70.312%)\n",
      " * Prec 69.670% \n",
      "best acc: 72.820000\n",
      "Epoch: [29][0/391]\tTime 0.569 (0.569)\tData 0.499 (0.499)\tLoss 0.6188 (0.6188)\tPrec 76.562% (76.562%)\n",
      "Epoch: [29][100/391]\tTime 0.052 (0.054)\tData 0.003 (0.007)\tLoss 0.5623 (0.6847)\tPrec 81.250% (75.750%)\n",
      "Epoch: [29][200/391]\tTime 0.049 (0.051)\tData 0.002 (0.005)\tLoss 0.7142 (0.6918)\tPrec 73.438% (75.556%)\n",
      "Epoch: [29][300/391]\tTime 0.044 (0.050)\tData 0.002 (0.004)\tLoss 0.8663 (0.6944)\tPrec 72.656% (75.446%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.351 (0.351)\tLoss 0.7981 (0.7981)\tPrec 72.656% (72.656%)\n",
      " * Prec 74.810% \n",
      "best acc: 74.810000\n",
      "Epoch: [30][0/391]\tTime 0.716 (0.716)\tData 0.647 (0.647)\tLoss 0.6335 (0.6335)\tPrec 78.125% (78.125%)\n",
      "Epoch: [30][100/391]\tTime 0.053 (0.065)\tData 0.002 (0.009)\tLoss 0.5831 (0.6579)\tPrec 80.469% (76.709%)\n",
      "Epoch: [30][200/391]\tTime 0.053 (0.060)\tData 0.002 (0.005)\tLoss 0.6427 (0.6684)\tPrec 75.781% (76.302%)\n",
      "Epoch: [30][300/391]\tTime 0.058 (0.058)\tData 0.002 (0.004)\tLoss 0.6390 (0.6669)\tPrec 78.125% (76.417%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.411 (0.411)\tLoss 0.7136 (0.7136)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.190% \n",
      "best acc: 76.190000\n",
      "Epoch: [31][0/391]\tTime 0.750 (0.750)\tData 0.682 (0.682)\tLoss 0.8383 (0.8383)\tPrec 71.094% (71.094%)\n",
      "Epoch: [31][100/391]\tTime 0.054 (0.056)\tData 0.003 (0.009)\tLoss 0.7257 (0.6544)\tPrec 76.562% (77.328%)\n",
      "Epoch: [31][200/391]\tTime 0.049 (0.052)\tData 0.003 (0.006)\tLoss 0.6030 (0.6580)\tPrec 82.812% (76.943%)\n",
      "Epoch: [31][300/391]\tTime 0.042 (0.051)\tData 0.002 (0.004)\tLoss 0.5807 (0.6576)\tPrec 81.250% (77.035%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.438 (0.438)\tLoss 0.7858 (0.7858)\tPrec 71.875% (71.875%)\n",
      " * Prec 73.100% \n",
      "best acc: 76.190000\n",
      "Epoch: [32][0/391]\tTime 0.629 (0.629)\tData 0.563 (0.563)\tLoss 0.7103 (0.7103)\tPrec 75.781% (75.781%)\n",
      "Epoch: [32][100/391]\tTime 0.055 (0.054)\tData 0.002 (0.008)\tLoss 0.6236 (0.6481)\tPrec 74.219% (77.158%)\n",
      "Epoch: [32][200/391]\tTime 0.043 (0.051)\tData 0.002 (0.005)\tLoss 0.6395 (0.6459)\tPrec 71.094% (77.243%)\n",
      "Epoch: [32][300/391]\tTime 0.046 (0.050)\tData 0.002 (0.004)\tLoss 0.6145 (0.6442)\tPrec 79.688% (77.346%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.355 (0.355)\tLoss 0.7521 (0.7521)\tPrec 75.000% (75.000%)\n",
      " * Prec 75.390% \n",
      "best acc: 76.190000\n",
      "Epoch: [33][0/391]\tTime 0.653 (0.653)\tData 0.586 (0.586)\tLoss 0.6339 (0.6339)\tPrec 78.125% (78.125%)\n",
      "Epoch: [33][100/391]\tTime 0.047 (0.054)\tData 0.002 (0.008)\tLoss 0.5012 (0.6259)\tPrec 83.594% (77.777%)\n",
      "Epoch: [33][200/391]\tTime 0.050 (0.051)\tData 0.002 (0.005)\tLoss 0.6518 (0.6303)\tPrec 78.125% (77.795%)\n",
      "Epoch: [33][300/391]\tTime 0.051 (0.050)\tData 0.002 (0.004)\tLoss 0.6305 (0.6384)\tPrec 76.562% (77.494%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.341 (0.341)\tLoss 0.7285 (0.7285)\tPrec 75.781% (75.781%)\n",
      " * Prec 76.750% \n",
      "best acc: 76.750000\n",
      "Epoch: [34][0/391]\tTime 0.658 (0.658)\tData 0.587 (0.587)\tLoss 0.6457 (0.6457)\tPrec 78.125% (78.125%)\n",
      "Epoch: [34][100/391]\tTime 0.043 (0.054)\tData 0.002 (0.008)\tLoss 0.5807 (0.6120)\tPrec 82.812% (78.535%)\n",
      "Epoch: [34][200/391]\tTime 0.043 (0.050)\tData 0.002 (0.005)\tLoss 0.7372 (0.6157)\tPrec 76.562% (78.599%)\n",
      "Epoch: [34][300/391]\tTime 0.055 (0.050)\tData 0.003 (0.004)\tLoss 0.6823 (0.6193)\tPrec 76.562% (78.366%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.357 (0.357)\tLoss 0.7275 (0.7275)\tPrec 73.438% (73.438%)\n",
      " * Prec 76.810% \n",
      "best acc: 76.810000\n",
      "Epoch: [35][0/391]\tTime 0.720 (0.720)\tData 0.653 (0.653)\tLoss 0.5864 (0.5864)\tPrec 78.906% (78.906%)\n",
      "Epoch: [35][100/391]\tTime 0.044 (0.054)\tData 0.002 (0.008)\tLoss 0.7012 (0.6198)\tPrec 76.562% (78.334%)\n",
      "Epoch: [35][200/391]\tTime 0.045 (0.052)\tData 0.002 (0.005)\tLoss 0.6547 (0.6068)\tPrec 75.781% (78.848%)\n",
      "Epoch: [35][300/391]\tTime 0.046 (0.051)\tData 0.002 (0.004)\tLoss 0.6453 (0.6040)\tPrec 75.000% (78.800%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.407 (0.407)\tLoss 0.7412 (0.7412)\tPrec 72.656% (72.656%)\n",
      " * Prec 77.520% \n",
      "best acc: 77.520000\n",
      "Epoch: [36][0/391]\tTime 0.819 (0.819)\tData 0.749 (0.749)\tLoss 0.5350 (0.5350)\tPrec 79.688% (79.688%)\n",
      "Epoch: [36][100/391]\tTime 0.054 (0.065)\tData 0.003 (0.010)\tLoss 0.6649 (0.5976)\tPrec 77.344% (79.030%)\n",
      "Epoch: [36][200/391]\tTime 0.049 (0.062)\tData 0.003 (0.006)\tLoss 0.7609 (0.6003)\tPrec 72.656% (79.035%)\n",
      "Epoch: [36][300/391]\tTime 0.056 (0.061)\tData 0.003 (0.005)\tLoss 0.5451 (0.6017)\tPrec 82.031% (79.013%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.413 (0.413)\tLoss 0.7702 (0.7702)\tPrec 76.562% (76.562%)\n",
      " * Prec 75.980% \n",
      "best acc: 77.520000\n",
      "Epoch: [37][0/391]\tTime 0.821 (0.821)\tData 0.762 (0.762)\tLoss 0.5922 (0.5922)\tPrec 75.781% (75.781%)\n",
      "Epoch: [37][100/391]\tTime 0.039 (0.047)\tData 0.001 (0.009)\tLoss 0.6004 (0.5951)\tPrec 78.125% (79.177%)\n",
      "Epoch: [37][200/391]\tTime 0.040 (0.044)\tData 0.002 (0.005)\tLoss 0.5578 (0.5893)\tPrec 80.469% (79.361%)\n",
      "Epoch: [37][300/391]\tTime 0.044 (0.044)\tData 0.002 (0.004)\tLoss 0.5856 (0.5939)\tPrec 80.469% (79.350%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.347 (0.347)\tLoss 0.6937 (0.6937)\tPrec 75.000% (75.000%)\n",
      " * Prec 76.470% \n",
      "best acc: 77.520000\n",
      "Epoch: [38][0/391]\tTime 0.633 (0.633)\tData 0.569 (0.569)\tLoss 0.6624 (0.6624)\tPrec 80.469% (80.469%)\n",
      "Epoch: [38][100/391]\tTime 0.049 (0.052)\tData 0.002 (0.008)\tLoss 0.6093 (0.5784)\tPrec 75.781% (79.734%)\n",
      "Epoch: [38][200/391]\tTime 0.047 (0.050)\tData 0.002 (0.005)\tLoss 0.7966 (0.5779)\tPrec 74.219% (79.925%)\n",
      "Epoch: [38][300/391]\tTime 0.049 (0.049)\tData 0.002 (0.004)\tLoss 0.6126 (0.5817)\tPrec 78.125% (79.745%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.383 (0.383)\tLoss 0.6685 (0.6685)\tPrec 76.562% (76.562%)\n",
      " * Prec 78.000% \n",
      "best acc: 78.000000\n",
      "Epoch: [39][0/391]\tTime 0.650 (0.650)\tData 0.595 (0.595)\tLoss 0.5710 (0.5710)\tPrec 81.250% (81.250%)\n",
      "Epoch: [39][100/391]\tTime 0.039 (0.046)\tData 0.001 (0.007)\tLoss 0.5575 (0.5709)\tPrec 80.469% (79.981%)\n",
      "Epoch: [39][200/391]\tTime 0.041 (0.043)\tData 0.001 (0.004)\tLoss 0.5363 (0.5700)\tPrec 82.031% (80.100%)\n",
      "Epoch: [39][300/391]\tTime 0.037 (0.042)\tData 0.002 (0.003)\tLoss 0.7999 (0.5735)\tPrec 72.656% (79.976%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.459 (0.459)\tLoss 0.6011 (0.6011)\tPrec 77.344% (77.344%)\n",
      " * Prec 78.930% \n",
      "best acc: 78.930000\n",
      "Epoch: [40][0/391]\tTime 0.630 (0.630)\tData 0.574 (0.574)\tLoss 0.6718 (0.6718)\tPrec 76.562% (76.562%)\n",
      "Epoch: [40][100/391]\tTime 0.054 (0.055)\tData 0.002 (0.008)\tLoss 0.4293 (0.5625)\tPrec 87.500% (80.376%)\n",
      "Epoch: [40][200/391]\tTime 0.056 (0.051)\tData 0.003 (0.005)\tLoss 0.5009 (0.5612)\tPrec 82.031% (80.593%)\n",
      "Epoch: [40][300/391]\tTime 0.061 (0.052)\tData 0.003 (0.004)\tLoss 0.5447 (0.5632)\tPrec 78.906% (80.560%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.731 (0.731)\tLoss 0.6816 (0.6816)\tPrec 78.125% (78.125%)\n",
      " * Prec 78.270% \n",
      "best acc: 78.930000\n",
      "Epoch: [41][0/391]\tTime 0.883 (0.883)\tData 0.813 (0.813)\tLoss 0.5384 (0.5384)\tPrec 78.125% (78.125%)\n",
      "Epoch: [41][100/391]\tTime 0.060 (0.067)\tData 0.002 (0.010)\tLoss 0.5238 (0.5569)\tPrec 81.250% (80.817%)\n",
      "Epoch: [41][200/391]\tTime 0.056 (0.062)\tData 0.003 (0.006)\tLoss 0.7464 (0.5587)\tPrec 77.344% (80.663%)\n",
      "Epoch: [41][300/391]\tTime 0.050 (0.061)\tData 0.002 (0.005)\tLoss 0.5457 (0.5575)\tPrec 78.906% (80.663%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.459 (0.459)\tLoss 0.7294 (0.7294)\tPrec 75.781% (75.781%)\n",
      " * Prec 78.870% \n",
      "best acc: 78.930000\n",
      "Epoch: [42][0/391]\tTime 0.720 (0.720)\tData 0.649 (0.649)\tLoss 0.5157 (0.5157)\tPrec 83.594% (83.594%)\n",
      "Epoch: [42][100/391]\tTime 0.051 (0.059)\tData 0.002 (0.009)\tLoss 0.5819 (0.5336)\tPrec 79.688% (81.513%)\n",
      "Epoch: [42][200/391]\tTime 0.049 (0.056)\tData 0.002 (0.006)\tLoss 0.5309 (0.5411)\tPrec 84.375% (81.219%)\n",
      "Epoch: [42][300/391]\tTime 0.051 (0.054)\tData 0.002 (0.005)\tLoss 0.6308 (0.5468)\tPrec 79.688% (80.980%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.318 (0.318)\tLoss 0.7562 (0.7562)\tPrec 72.656% (72.656%)\n",
      " * Prec 78.660% \n",
      "best acc: 78.930000\n",
      "Epoch: [43][0/391]\tTime 0.653 (0.653)\tData 0.583 (0.583)\tLoss 0.5034 (0.5034)\tPrec 83.594% (83.594%)\n",
      "Epoch: [43][100/391]\tTime 0.053 (0.054)\tData 0.002 (0.008)\tLoss 0.4540 (0.5508)\tPrec 84.375% (80.647%)\n",
      "Epoch: [43][200/391]\tTime 0.055 (0.051)\tData 0.002 (0.005)\tLoss 0.8071 (0.5449)\tPrec 80.469% (80.939%)\n",
      "Epoch: [43][300/391]\tTime 0.046 (0.050)\tData 0.002 (0.004)\tLoss 0.5598 (0.5383)\tPrec 82.031% (81.128%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.336 (0.336)\tLoss 0.6152 (0.6152)\tPrec 78.125% (78.125%)\n",
      " * Prec 78.990% \n",
      "best acc: 78.990000\n",
      "Epoch: [44][0/391]\tTime 0.799 (0.799)\tData 0.730 (0.730)\tLoss 0.4790 (0.4790)\tPrec 85.938% (85.938%)\n",
      "Epoch: [44][100/391]\tTime 0.061 (0.065)\tData 0.003 (0.010)\tLoss 0.5825 (0.5287)\tPrec 78.125% (81.459%)\n",
      "Epoch: [44][200/391]\tTime 0.049 (0.061)\tData 0.002 (0.006)\tLoss 0.5974 (0.5236)\tPrec 80.469% (81.748%)\n",
      "Epoch: [44][300/391]\tTime 0.043 (0.057)\tData 0.001 (0.005)\tLoss 0.6383 (0.5296)\tPrec 76.562% (81.632%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.296 (0.296)\tLoss 0.5824 (0.5824)\tPrec 80.469% (80.469%)\n",
      " * Prec 78.320% \n",
      "best acc: 78.990000\n",
      "Epoch: [45][0/391]\tTime 0.644 (0.644)\tData 0.572 (0.572)\tLoss 0.4938 (0.4938)\tPrec 82.031% (82.031%)\n",
      "Epoch: [45][100/391]\tTime 0.053 (0.053)\tData 0.002 (0.008)\tLoss 0.6516 (0.5178)\tPrec 78.906% (82.101%)\n",
      "Epoch: [45][200/391]\tTime 0.048 (0.051)\tData 0.002 (0.005)\tLoss 0.6192 (0.5271)\tPrec 78.906% (81.705%)\n",
      "Epoch: [45][300/391]\tTime 0.044 (0.050)\tData 0.002 (0.004)\tLoss 0.6464 (0.5281)\tPrec 76.562% (81.595%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.313 (0.313)\tLoss 0.5552 (0.5552)\tPrec 82.031% (82.031%)\n",
      " * Prec 79.320% \n",
      "best acc: 79.320000\n",
      "Epoch: [46][0/391]\tTime 0.659 (0.659)\tData 0.591 (0.591)\tLoss 0.4864 (0.4864)\tPrec 86.719% (86.719%)\n",
      "Epoch: [46][100/391]\tTime 0.056 (0.057)\tData 0.002 (0.008)\tLoss 0.4865 (0.5093)\tPrec 82.812% (82.132%)\n",
      "Epoch: [46][200/391]\tTime 0.061 (0.053)\tData 0.003 (0.005)\tLoss 0.5133 (0.5144)\tPrec 83.594% (82.132%)\n",
      "Epoch: [46][300/391]\tTime 0.062 (0.053)\tData 0.002 (0.004)\tLoss 0.3572 (0.5171)\tPrec 89.062% (82.000%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.494 (0.494)\tLoss 0.8070 (0.8070)\tPrec 72.656% (72.656%)\n",
      " * Prec 77.400% \n",
      "best acc: 79.320000\n",
      "Epoch: [47][0/391]\tTime 0.689 (0.689)\tData 0.633 (0.633)\tLoss 0.4116 (0.4116)\tPrec 87.500% (87.500%)\n",
      "Epoch: [47][100/391]\tTime 0.060 (0.065)\tData 0.002 (0.009)\tLoss 0.4828 (0.5267)\tPrec 85.156% (81.923%)\n",
      "Epoch: [47][200/391]\tTime 0.061 (0.061)\tData 0.003 (0.005)\tLoss 0.4666 (0.5160)\tPrec 83.594% (82.062%)\n",
      "Epoch: [47][300/391]\tTime 0.056 (0.060)\tData 0.003 (0.004)\tLoss 0.5733 (0.5147)\tPrec 78.906% (82.164%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.289 (0.289)\tLoss 0.6313 (0.6313)\tPrec 79.688% (79.688%)\n",
      " * Prec 80.180% \n",
      "best acc: 80.180000\n",
      "Epoch: [48][0/391]\tTime 0.755 (0.755)\tData 0.688 (0.688)\tLoss 0.5283 (0.5283)\tPrec 82.031% (82.031%)\n",
      "Epoch: [48][100/391]\tTime 0.057 (0.058)\tData 0.002 (0.009)\tLoss 0.5622 (0.5062)\tPrec 79.688% (82.542%)\n",
      "Epoch: [48][200/391]\tTime 0.069 (0.057)\tData 0.003 (0.006)\tLoss 0.6032 (0.5036)\tPrec 75.781% (82.537%)\n",
      "Epoch: [48][300/391]\tTime 0.063 (0.059)\tData 0.002 (0.005)\tLoss 0.5777 (0.5048)\tPrec 78.906% (82.488%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.5238 (0.5238)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.480% \n",
      "best acc: 80.180000\n",
      "Epoch: [49][0/391]\tTime 0.740 (0.740)\tData 0.668 (0.668)\tLoss 0.4880 (0.4880)\tPrec 82.812% (82.812%)\n",
      "Epoch: [49][100/391]\tTime 0.061 (0.065)\tData 0.002 (0.009)\tLoss 0.4178 (0.4805)\tPrec 84.375% (83.393%)\n",
      "Epoch: [49][200/391]\tTime 0.061 (0.063)\tData 0.002 (0.006)\tLoss 0.5698 (0.4906)\tPrec 79.688% (82.746%)\n",
      "Epoch: [49][300/391]\tTime 0.061 (0.062)\tData 0.002 (0.004)\tLoss 0.5762 (0.4918)\tPrec 78.906% (82.729%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.390 (0.390)\tLoss 0.5691 (0.5691)\tPrec 82.031% (82.031%)\n",
      " * Prec 80.260% \n",
      "best acc: 80.260000\n",
      "Epoch: [50][0/391]\tTime 0.616 (0.616)\tData 0.545 (0.545)\tLoss 0.5470 (0.5470)\tPrec 81.250% (81.250%)\n",
      "Epoch: [50][100/391]\tTime 0.055 (0.059)\tData 0.002 (0.008)\tLoss 0.6610 (0.4916)\tPrec 78.906% (82.758%)\n",
      "Epoch: [50][200/391]\tTime 0.049 (0.058)\tData 0.002 (0.005)\tLoss 0.4219 (0.4860)\tPrec 84.375% (82.995%)\n",
      "Epoch: [50][300/391]\tTime 0.050 (0.056)\tData 0.002 (0.004)\tLoss 0.3908 (0.4876)\tPrec 87.500% (82.864%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.258 (0.258)\tLoss 0.6142 (0.6142)\tPrec 79.688% (79.688%)\n",
      " * Prec 80.650% \n",
      "best acc: 80.650000\n",
      "Epoch: [51][0/391]\tTime 0.662 (0.662)\tData 0.589 (0.589)\tLoss 0.5159 (0.5159)\tPrec 82.812% (82.812%)\n",
      "Epoch: [51][100/391]\tTime 0.058 (0.059)\tData 0.002 (0.008)\tLoss 0.5015 (0.4954)\tPrec 79.688% (82.967%)\n",
      "Epoch: [51][200/391]\tTime 0.056 (0.055)\tData 0.003 (0.005)\tLoss 0.4705 (0.4924)\tPrec 82.812% (82.844%)\n",
      "Epoch: [51][300/391]\tTime 0.060 (0.054)\tData 0.002 (0.004)\tLoss 0.4050 (0.4917)\tPrec 83.594% (82.831%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.320 (0.320)\tLoss 0.6879 (0.6879)\tPrec 79.688% (79.688%)\n",
      " * Prec 78.070% \n",
      "best acc: 80.650000\n",
      "Epoch: [52][0/391]\tTime 0.669 (0.669)\tData 0.591 (0.591)\tLoss 0.5956 (0.5956)\tPrec 79.688% (79.688%)\n",
      "Epoch: [52][100/391]\tTime 0.054 (0.062)\tData 0.002 (0.008)\tLoss 0.4415 (0.4867)\tPrec 84.375% (83.269%)\n",
      "Epoch: [52][200/391]\tTime 0.060 (0.057)\tData 0.002 (0.005)\tLoss 0.5854 (0.4885)\tPrec 82.031% (83.190%)\n",
      "Epoch: [52][300/391]\tTime 0.061 (0.056)\tData 0.002 (0.004)\tLoss 0.4443 (0.4868)\tPrec 88.281% (83.168%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.5459 (0.5459)\tPrec 79.688% (79.688%)\n",
      " * Prec 79.340% \n",
      "best acc: 80.650000\n",
      "Epoch: [53][0/391]\tTime 0.383 (0.383)\tData 0.308 (0.308)\tLoss 0.4408 (0.4408)\tPrec 84.375% (84.375%)\n",
      "Epoch: [53][100/391]\tTime 0.050 (0.060)\tData 0.002 (0.005)\tLoss 0.4764 (0.4663)\tPrec 83.594% (83.687%)\n",
      "Epoch: [53][200/391]\tTime 0.055 (0.057)\tData 0.002 (0.004)\tLoss 0.4691 (0.4724)\tPrec 79.688% (83.454%)\n",
      "Epoch: [53][300/391]\tTime 0.054 (0.055)\tData 0.002 (0.003)\tLoss 0.3669 (0.4801)\tPrec 88.281% (83.267%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.380 (0.380)\tLoss 0.6055 (0.6055)\tPrec 76.562% (76.562%)\n",
      " * Prec 81.130% \n",
      "best acc: 81.130000\n",
      "Epoch: [54][0/391]\tTime 0.721 (0.721)\tData 0.645 (0.645)\tLoss 0.3749 (0.3749)\tPrec 82.812% (82.812%)\n",
      "Epoch: [54][100/391]\tTime 0.045 (0.060)\tData 0.002 (0.009)\tLoss 0.4274 (0.4612)\tPrec 82.031% (83.679%)\n",
      "Epoch: [54][200/391]\tTime 0.051 (0.056)\tData 0.002 (0.005)\tLoss 0.4970 (0.4610)\tPrec 82.812% (83.726%)\n",
      "Epoch: [54][300/391]\tTime 0.064 (0.056)\tData 0.002 (0.004)\tLoss 0.5093 (0.4643)\tPrec 82.031% (83.698%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.407 (0.407)\tLoss 0.7824 (0.7824)\tPrec 75.781% (75.781%)\n",
      " * Prec 77.610% \n",
      "best acc: 81.130000\n",
      "Epoch: [55][0/391]\tTime 0.625 (0.625)\tData 0.543 (0.543)\tLoss 0.4142 (0.4142)\tPrec 85.156% (85.156%)\n",
      "Epoch: [55][100/391]\tTime 0.046 (0.059)\tData 0.002 (0.008)\tLoss 0.5467 (0.4753)\tPrec 78.125% (83.269%)\n",
      "Epoch: [55][200/391]\tTime 0.046 (0.054)\tData 0.002 (0.005)\tLoss 0.3785 (0.4707)\tPrec 86.719% (83.570%)\n",
      "Epoch: [55][300/391]\tTime 0.046 (0.052)\tData 0.002 (0.004)\tLoss 0.4503 (0.4724)\tPrec 85.156% (83.480%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 0.5973 (0.5973)\tPrec 78.906% (78.906%)\n",
      " * Prec 80.930% \n",
      "best acc: 81.130000\n",
      "Epoch: [56][0/391]\tTime 0.656 (0.656)\tData 0.580 (0.580)\tLoss 0.3595 (0.3595)\tPrec 87.500% (87.500%)\n",
      "Epoch: [56][100/391]\tTime 0.049 (0.057)\tData 0.002 (0.008)\tLoss 0.6421 (0.4540)\tPrec 77.344% (84.182%)\n",
      "Epoch: [56][200/391]\tTime 0.045 (0.055)\tData 0.002 (0.005)\tLoss 0.4753 (0.4619)\tPrec 84.375% (83.854%)\n",
      "Epoch: [56][300/391]\tTime 0.059 (0.054)\tData 0.002 (0.004)\tLoss 0.2995 (0.4610)\tPrec 89.844% (83.962%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 0.6674 (0.6674)\tPrec 76.562% (76.562%)\n",
      " * Prec 80.450% \n",
      "best acc: 81.130000\n",
      "Epoch: [57][0/391]\tTime 0.702 (0.702)\tData 0.631 (0.631)\tLoss 0.4768 (0.4768)\tPrec 82.812% (82.812%)\n",
      "Epoch: [57][100/391]\tTime 0.059 (0.060)\tData 0.003 (0.008)\tLoss 0.4141 (0.4371)\tPrec 84.375% (84.692%)\n",
      "Epoch: [57][200/391]\tTime 0.061 (0.057)\tData 0.003 (0.005)\tLoss 0.4561 (0.4527)\tPrec 84.375% (84.064%)\n",
      "Epoch: [57][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.004)\tLoss 0.4920 (0.4544)\tPrec 85.156% (84.082%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.370 (0.370)\tLoss 0.6741 (0.6741)\tPrec 74.219% (74.219%)\n",
      " * Prec 79.760% \n",
      "best acc: 81.130000\n",
      "Epoch: [58][0/391]\tTime 0.741 (0.741)\tData 0.662 (0.662)\tLoss 0.4364 (0.4364)\tPrec 84.375% (84.375%)\n",
      "Epoch: [58][100/391]\tTime 0.057 (0.061)\tData 0.003 (0.009)\tLoss 0.3940 (0.4438)\tPrec 87.500% (84.066%)\n",
      "Epoch: [58][200/391]\tTime 0.049 (0.056)\tData 0.002 (0.005)\tLoss 0.3529 (0.4416)\tPrec 86.719% (84.336%)\n",
      "Epoch: [58][300/391]\tTime 0.063 (0.055)\tData 0.002 (0.004)\tLoss 0.5772 (0.4486)\tPrec 77.344% (84.186%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.320 (0.320)\tLoss 0.5925 (0.5925)\tPrec 74.219% (74.219%)\n",
      " * Prec 79.930% \n",
      "best acc: 81.130000\n",
      "Epoch: [59][0/391]\tTime 0.618 (0.618)\tData 0.546 (0.546)\tLoss 0.3913 (0.3913)\tPrec 83.594% (83.594%)\n",
      "Epoch: [59][100/391]\tTime 0.047 (0.059)\tData 0.002 (0.008)\tLoss 0.4014 (0.4534)\tPrec 86.719% (84.112%)\n",
      "Epoch: [59][200/391]\tTime 0.054 (0.055)\tData 0.002 (0.005)\tLoss 0.4255 (0.4489)\tPrec 84.375% (84.387%)\n",
      "Epoch: [59][300/391]\tTime 0.044 (0.055)\tData 0.002 (0.004)\tLoss 0.2814 (0.4514)\tPrec 88.281% (84.326%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.6677 (0.6677)\tPrec 79.688% (79.688%)\n",
      " * Prec 80.760% \n",
      "best acc: 81.130000\n",
      "Epoch: [60][0/391]\tTime 0.660 (0.660)\tData 0.588 (0.588)\tLoss 0.5248 (0.5248)\tPrec 81.250% (81.250%)\n",
      "Epoch: [60][100/391]\tTime 0.064 (0.059)\tData 0.002 (0.008)\tLoss 0.5255 (0.4525)\tPrec 82.031% (84.329%)\n",
      "Epoch: [60][200/391]\tTime 0.045 (0.056)\tData 0.002 (0.005)\tLoss 0.4028 (0.4417)\tPrec 84.375% (84.869%)\n",
      "Epoch: [60][300/391]\tTime 0.051 (0.055)\tData 0.002 (0.004)\tLoss 0.4546 (0.4460)\tPrec 82.031% (84.609%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.175 (0.175)\tLoss 0.5723 (0.5723)\tPrec 76.562% (76.562%)\n",
      " * Prec 78.880% \n",
      "best acc: 81.130000\n",
      "Epoch: [61][0/391]\tTime 0.814 (0.814)\tData 0.744 (0.744)\tLoss 0.3624 (0.3624)\tPrec 88.281% (88.281%)\n",
      "Epoch: [61][100/391]\tTime 0.046 (0.060)\tData 0.002 (0.009)\tLoss 0.4542 (0.4405)\tPrec 82.812% (84.878%)\n",
      "Epoch: [61][200/391]\tTime 0.056 (0.056)\tData 0.003 (0.006)\tLoss 0.4277 (0.4421)\tPrec 83.594% (84.694%)\n",
      "Epoch: [61][300/391]\tTime 0.046 (0.055)\tData 0.002 (0.005)\tLoss 0.5352 (0.4433)\tPrec 85.156% (84.692%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.347 (0.347)\tLoss 0.6819 (0.6819)\tPrec 79.688% (79.688%)\n",
      " * Prec 78.880% \n",
      "best acc: 81.130000\n",
      "Epoch: [62][0/391]\tTime 0.634 (0.634)\tData 0.557 (0.557)\tLoss 0.4290 (0.4290)\tPrec 85.938% (85.938%)\n",
      "Epoch: [62][100/391]\tTime 0.051 (0.065)\tData 0.002 (0.008)\tLoss 0.4920 (0.4303)\tPrec 82.031% (84.886%)\n",
      "Epoch: [62][200/391]\tTime 0.067 (0.060)\tData 0.003 (0.005)\tLoss 0.4603 (0.4388)\tPrec 83.594% (84.480%)\n",
      "Epoch: [62][300/391]\tTime 0.049 (0.057)\tData 0.002 (0.004)\tLoss 0.5221 (0.4407)\tPrec 85.938% (84.352%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.7293 (0.7293)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.780% \n",
      "best acc: 81.130000\n",
      "Epoch: [63][0/391]\tTime 0.853 (0.853)\tData 0.779 (0.779)\tLoss 0.4077 (0.4077)\tPrec 86.719% (86.719%)\n",
      "Epoch: [63][100/391]\tTime 0.060 (0.065)\tData 0.002 (0.010)\tLoss 0.4700 (0.4272)\tPrec 85.156% (85.373%)\n",
      "Epoch: [63][200/391]\tTime 0.051 (0.060)\tData 0.002 (0.006)\tLoss 0.3861 (0.4321)\tPrec 85.938% (85.063%)\n",
      "Epoch: [63][300/391]\tTime 0.040 (0.058)\tData 0.002 (0.005)\tLoss 0.4200 (0.4360)\tPrec 84.375% (84.936%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 0.5719 (0.5719)\tPrec 78.906% (78.906%)\n",
      " * Prec 81.390% \n",
      "best acc: 81.390000\n",
      "Epoch: [64][0/391]\tTime 0.729 (0.729)\tData 0.655 (0.655)\tLoss 0.4600 (0.4600)\tPrec 82.031% (82.031%)\n",
      "Epoch: [64][100/391]\tTime 0.056 (0.060)\tData 0.002 (0.009)\tLoss 0.5204 (0.4325)\tPrec 82.812% (85.009%)\n",
      "Epoch: [64][200/391]\tTime 0.053 (0.056)\tData 0.002 (0.005)\tLoss 0.3360 (0.4246)\tPrec 90.625% (85.207%)\n",
      "Epoch: [64][300/391]\tTime 0.050 (0.053)\tData 0.002 (0.004)\tLoss 0.4182 (0.4278)\tPrec 84.375% (85.180%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.507 (0.507)\tLoss 0.5970 (0.5970)\tPrec 81.250% (81.250%)\n",
      " * Prec 80.580% \n",
      "best acc: 81.390000\n",
      "Epoch: [65][0/391]\tTime 0.632 (0.632)\tData 0.561 (0.561)\tLoss 0.2930 (0.2930)\tPrec 90.625% (90.625%)\n",
      "Epoch: [65][100/391]\tTime 0.044 (0.054)\tData 0.003 (0.008)\tLoss 0.3681 (0.4159)\tPrec 88.281% (85.357%)\n",
      "Epoch: [65][200/391]\tTime 0.042 (0.052)\tData 0.002 (0.005)\tLoss 0.3960 (0.4123)\tPrec 86.719% (85.595%)\n",
      "Epoch: [65][300/391]\tTime 0.044 (0.051)\tData 0.002 (0.004)\tLoss 0.5154 (0.4180)\tPrec 83.594% (85.418%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.508 (0.508)\tLoss 0.6578 (0.6578)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.980% \n",
      "best acc: 81.390000\n",
      "Epoch: [66][0/391]\tTime 0.762 (0.762)\tData 0.686 (0.686)\tLoss 0.4823 (0.4823)\tPrec 82.812% (82.812%)\n",
      "Epoch: [66][100/391]\tTime 0.061 (0.064)\tData 0.002 (0.009)\tLoss 0.4923 (0.4269)\tPrec 82.031% (84.886%)\n",
      "Epoch: [66][200/391]\tTime 0.062 (0.061)\tData 0.003 (0.006)\tLoss 0.5381 (0.4234)\tPrec 82.031% (85.024%)\n",
      "Epoch: [66][300/391]\tTime 0.053 (0.060)\tData 0.002 (0.005)\tLoss 0.3740 (0.4225)\tPrec 85.938% (85.091%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.393 (0.393)\tLoss 0.5830 (0.5830)\tPrec 78.906% (78.906%)\n",
      " * Prec 80.320% \n",
      "best acc: 81.390000\n",
      "Epoch: [67][0/391]\tTime 0.622 (0.622)\tData 0.546 (0.546)\tLoss 0.4314 (0.4314)\tPrec 85.938% (85.938%)\n",
      "Epoch: [67][100/391]\tTime 0.059 (0.060)\tData 0.002 (0.008)\tLoss 0.4975 (0.4208)\tPrec 81.250% (85.412%)\n",
      "Epoch: [67][200/391]\tTime 0.062 (0.058)\tData 0.002 (0.005)\tLoss 0.3472 (0.4189)\tPrec 88.281% (85.557%)\n",
      "Epoch: [67][300/391]\tTime 0.055 (0.058)\tData 0.003 (0.004)\tLoss 0.3859 (0.4149)\tPrec 86.719% (85.678%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.287 (0.287)\tLoss 0.5795 (0.5795)\tPrec 78.125% (78.125%)\n",
      " * Prec 81.710% \n",
      "best acc: 81.710000\n",
      "Epoch: [68][0/391]\tTime 0.622 (0.622)\tData 0.555 (0.555)\tLoss 0.3622 (0.3622)\tPrec 86.719% (86.719%)\n",
      "Epoch: [68][100/391]\tTime 0.054 (0.065)\tData 0.002 (0.008)\tLoss 0.3988 (0.4097)\tPrec 82.812% (85.868%)\n",
      "Epoch: [68][200/391]\tTime 0.049 (0.059)\tData 0.002 (0.005)\tLoss 0.3925 (0.4120)\tPrec 85.938% (85.840%)\n",
      "Epoch: [68][300/391]\tTime 0.056 (0.056)\tData 0.002 (0.004)\tLoss 0.4362 (0.4125)\tPrec 85.156% (85.727%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.284 (0.284)\tLoss 0.7302 (0.7302)\tPrec 77.344% (77.344%)\n",
      " * Prec 77.570% \n",
      "best acc: 81.710000\n",
      "Epoch: [69][0/391]\tTime 0.654 (0.654)\tData 0.584 (0.584)\tLoss 0.4443 (0.4443)\tPrec 83.594% (83.594%)\n",
      "Epoch: [69][100/391]\tTime 0.049 (0.058)\tData 0.002 (0.008)\tLoss 0.5321 (0.4075)\tPrec 82.812% (86.030%)\n",
      "Epoch: [69][200/391]\tTime 0.045 (0.056)\tData 0.002 (0.005)\tLoss 0.3439 (0.4067)\tPrec 87.500% (85.934%)\n",
      "Epoch: [69][300/391]\tTime 0.050 (0.054)\tData 0.002 (0.004)\tLoss 0.3348 (0.4058)\tPrec 85.938% (85.974%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.313 (0.313)\tLoss 0.6150 (0.6150)\tPrec 78.906% (78.906%)\n",
      " * Prec 81.260% \n",
      "best acc: 81.710000\n",
      "Epoch: [70][0/391]\tTime 0.602 (0.602)\tData 0.530 (0.530)\tLoss 0.3455 (0.3455)\tPrec 86.719% (86.719%)\n",
      "Epoch: [70][100/391]\tTime 0.044 (0.063)\tData 0.002 (0.007)\tLoss 0.3823 (0.4053)\tPrec 85.938% (85.752%)\n",
      "Epoch: [70][200/391]\tTime 0.050 (0.056)\tData 0.002 (0.005)\tLoss 0.4457 (0.4048)\tPrec 84.375% (85.840%)\n",
      "Epoch: [70][300/391]\tTime 0.045 (0.055)\tData 0.002 (0.004)\tLoss 0.3937 (0.4052)\tPrec 86.719% (85.935%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.439 (0.439)\tLoss 0.5524 (0.5524)\tPrec 81.250% (81.250%)\n",
      " * Prec 82.050% \n",
      "best acc: 82.050000\n",
      "Epoch: [71][0/391]\tTime 0.630 (0.630)\tData 0.552 (0.552)\tLoss 0.3871 (0.3871)\tPrec 83.594% (83.594%)\n",
      "Epoch: [71][100/391]\tTime 0.048 (0.058)\tData 0.002 (0.008)\tLoss 0.3194 (0.4003)\tPrec 89.844% (86.231%)\n",
      "Epoch: [71][200/391]\tTime 0.045 (0.053)\tData 0.001 (0.005)\tLoss 0.4196 (0.3937)\tPrec 82.812% (86.365%)\n",
      "Epoch: [71][300/391]\tTime 0.044 (0.051)\tData 0.002 (0.004)\tLoss 0.4416 (0.3998)\tPrec 85.156% (86.111%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.6373 (0.6373)\tPrec 78.125% (78.125%)\n",
      " * Prec 80.220% \n",
      "best acc: 82.050000\n",
      "Epoch: [72][0/391]\tTime 0.760 (0.760)\tData 0.687 (0.687)\tLoss 0.5010 (0.5010)\tPrec 83.594% (83.594%)\n",
      "Epoch: [72][100/391]\tTime 0.053 (0.065)\tData 0.002 (0.009)\tLoss 0.3609 (0.3965)\tPrec 89.844% (86.077%)\n",
      "Epoch: [72][200/391]\tTime 0.045 (0.061)\tData 0.002 (0.006)\tLoss 0.3480 (0.3967)\tPrec 89.062% (86.116%)\n",
      "Epoch: [72][300/391]\tTime 0.054 (0.057)\tData 0.003 (0.004)\tLoss 0.4241 (0.3987)\tPrec 84.375% (86.161%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.265 (0.265)\tLoss 0.6677 (0.6677)\tPrec 78.906% (78.906%)\n",
      " * Prec 80.350% \n",
      "best acc: 82.050000\n",
      "Epoch: [73][0/391]\tTime 0.718 (0.718)\tData 0.642 (0.642)\tLoss 0.4766 (0.4766)\tPrec 82.031% (82.031%)\n",
      "Epoch: [73][100/391]\tTime 0.050 (0.062)\tData 0.002 (0.009)\tLoss 0.4665 (0.3969)\tPrec 84.375% (86.069%)\n",
      "Epoch: [73][200/391]\tTime 0.054 (0.060)\tData 0.002 (0.006)\tLoss 0.5026 (0.3944)\tPrec 84.375% (86.213%)\n",
      "Epoch: [73][300/391]\tTime 0.046 (0.058)\tData 0.002 (0.004)\tLoss 0.5590 (0.3953)\tPrec 82.812% (86.132%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.318 (0.318)\tLoss 0.4830 (0.4830)\tPrec 82.812% (82.812%)\n",
      " * Prec 82.890% \n",
      "best acc: 82.890000\n",
      "Epoch: [74][0/391]\tTime 0.497 (0.497)\tData 0.417 (0.417)\tLoss 0.3248 (0.3248)\tPrec 87.500% (87.500%)\n",
      "Epoch: [74][100/391]\tTime 0.061 (0.063)\tData 0.002 (0.006)\tLoss 0.3223 (0.3889)\tPrec 86.719% (86.641%)\n",
      "Epoch: [74][200/391]\tTime 0.046 (0.060)\tData 0.002 (0.004)\tLoss 0.3770 (0.3889)\tPrec 86.719% (86.416%)\n",
      "Epoch: [74][300/391]\tTime 0.047 (0.058)\tData 0.002 (0.003)\tLoss 0.3896 (0.3904)\tPrec 85.156% (86.402%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.397 (0.397)\tLoss 0.5328 (0.5328)\tPrec 82.812% (82.812%)\n",
      " * Prec 80.540% \n",
      "best acc: 82.890000\n",
      "Epoch: [75][0/391]\tTime 1.067 (1.067)\tData 0.994 (0.994)\tLoss 0.2768 (0.2768)\tPrec 88.281% (88.281%)\n",
      "Epoch: [75][100/391]\tTime 0.064 (0.065)\tData 0.002 (0.012)\tLoss 0.4299 (0.3776)\tPrec 86.719% (87.013%)\n",
      "Epoch: [75][200/391]\tTime 0.060 (0.062)\tData 0.002 (0.007)\tLoss 0.3789 (0.3826)\tPrec 87.500% (86.765%)\n",
      "Epoch: [75][300/391]\tTime 0.061 (0.061)\tData 0.002 (0.005)\tLoss 0.4530 (0.3853)\tPrec 81.250% (86.589%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.6600 (0.6600)\tPrec 80.469% (80.469%)\n",
      " * Prec 80.680% \n",
      "best acc: 82.890000\n",
      "Epoch: [76][0/391]\tTime 0.872 (0.872)\tData 0.801 (0.801)\tLoss 0.3290 (0.3290)\tPrec 89.062% (89.062%)\n",
      "Epoch: [76][100/391]\tTime 0.057 (0.064)\tData 0.002 (0.010)\tLoss 0.3259 (0.3794)\tPrec 89.062% (86.471%)\n",
      "Epoch: [76][200/391]\tTime 0.060 (0.061)\tData 0.002 (0.006)\tLoss 0.3915 (0.3887)\tPrec 89.844% (86.400%)\n",
      "Epoch: [76][300/391]\tTime 0.061 (0.060)\tData 0.002 (0.005)\tLoss 0.3004 (0.3866)\tPrec 86.719% (86.389%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.5621 (0.5621)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.440% \n",
      "best acc: 82.890000\n",
      "Epoch: [77][0/391]\tTime 0.626 (0.626)\tData 0.556 (0.556)\tLoss 0.4171 (0.4171)\tPrec 85.938% (85.938%)\n",
      "Epoch: [77][100/391]\tTime 0.059 (0.059)\tData 0.003 (0.008)\tLoss 0.4105 (0.3720)\tPrec 85.156% (86.804%)\n",
      "Epoch: [77][200/391]\tTime 0.046 (0.057)\tData 0.002 (0.005)\tLoss 0.3474 (0.3781)\tPrec 87.500% (86.528%)\n",
      "Epoch: [77][300/391]\tTime 0.050 (0.055)\tData 0.002 (0.004)\tLoss 0.3226 (0.3813)\tPrec 88.281% (86.521%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.317 (0.317)\tLoss 0.5616 (0.5616)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.920% \n",
      "best acc: 82.890000\n",
      "Epoch: [78][0/391]\tTime 0.660 (0.660)\tData 0.585 (0.585)\tLoss 0.3298 (0.3298)\tPrec 88.281% (88.281%)\n",
      "Epoch: [78][100/391]\tTime 0.068 (0.059)\tData 0.003 (0.008)\tLoss 0.4574 (0.3828)\tPrec 83.594% (86.850%)\n",
      "Epoch: [78][200/391]\tTime 0.041 (0.055)\tData 0.002 (0.005)\tLoss 0.3540 (0.3791)\tPrec 85.938% (86.738%)\n",
      "Epoch: [78][300/391]\tTime 0.053 (0.053)\tData 0.002 (0.004)\tLoss 0.4693 (0.3789)\tPrec 84.375% (86.688%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.383 (0.383)\tLoss 0.4940 (0.4940)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.420% \n",
      "best acc: 82.890000\n",
      "Epoch: [79][0/391]\tTime 0.755 (0.755)\tData 0.686 (0.686)\tLoss 0.2716 (0.2716)\tPrec 92.188% (92.188%)\n",
      "Epoch: [79][100/391]\tTime 0.053 (0.061)\tData 0.002 (0.009)\tLoss 0.4868 (0.3714)\tPrec 85.156% (87.353%)\n",
      "Epoch: [79][200/391]\tTime 0.048 (0.057)\tData 0.002 (0.006)\tLoss 0.4096 (0.3779)\tPrec 86.719% (86.847%)\n",
      "Epoch: [79][300/391]\tTime 0.056 (0.056)\tData 0.002 (0.004)\tLoss 0.3327 (0.3752)\tPrec 89.844% (86.932%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.321 (0.321)\tLoss 0.5837 (0.5837)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.040% \n",
      "best acc: 83.040000\n",
      "Epoch: [80][0/391]\tTime 0.745 (0.745)\tData 0.671 (0.671)\tLoss 0.4112 (0.4112)\tPrec 85.156% (85.156%)\n",
      "Epoch: [80][100/391]\tTime 0.056 (0.059)\tData 0.002 (0.009)\tLoss 0.5114 (0.3606)\tPrec 82.031% (87.252%)\n",
      "Epoch: [80][200/391]\tTime 0.051 (0.056)\tData 0.002 (0.006)\tLoss 0.3802 (0.3705)\tPrec 86.719% (87.030%)\n",
      "Epoch: [80][300/391]\tTime 0.048 (0.053)\tData 0.002 (0.004)\tLoss 0.3700 (0.3742)\tPrec 89.844% (86.890%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.409 (0.409)\tLoss 0.5412 (0.5412)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.950% \n",
      "best acc: 83.040000\n",
      "Epoch: [81][0/391]\tTime 0.982 (0.982)\tData 0.895 (0.895)\tLoss 0.3957 (0.3957)\tPrec 87.500% (87.500%)\n",
      "Epoch: [81][100/391]\tTime 0.050 (0.062)\tData 0.002 (0.011)\tLoss 0.3410 (0.3687)\tPrec 88.281% (86.843%)\n",
      "Epoch: [81][200/391]\tTime 0.057 (0.058)\tData 0.002 (0.007)\tLoss 0.5566 (0.3688)\tPrec 82.031% (86.940%)\n",
      "Epoch: [81][300/391]\tTime 0.048 (0.056)\tData 0.002 (0.005)\tLoss 0.3117 (0.3674)\tPrec 90.625% (87.067%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.297 (0.297)\tLoss 0.4740 (0.4740)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.130% \n",
      "best acc: 83.130000\n",
      "Epoch: [82][0/391]\tTime 0.621 (0.621)\tData 0.547 (0.547)\tLoss 0.3158 (0.3158)\tPrec 89.844% (89.844%)\n",
      "Epoch: [82][100/391]\tTime 0.046 (0.063)\tData 0.002 (0.008)\tLoss 0.3232 (0.3659)\tPrec 91.406% (87.175%)\n",
      "Epoch: [82][200/391]\tTime 0.045 (0.056)\tData 0.002 (0.005)\tLoss 0.3556 (0.3700)\tPrec 84.375% (86.952%)\n",
      "Epoch: [82][300/391]\tTime 0.049 (0.054)\tData 0.002 (0.004)\tLoss 0.2656 (0.3671)\tPrec 89.844% (87.025%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.368 (0.368)\tLoss 0.4319 (0.4319)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.940% \n",
      "best acc: 83.130000\n",
      "Epoch: [83][0/391]\tTime 0.826 (0.826)\tData 0.753 (0.753)\tLoss 0.3041 (0.3041)\tPrec 90.625% (90.625%)\n",
      "Epoch: [83][100/391]\tTime 0.055 (0.055)\tData 0.005 (0.009)\tLoss 0.2503 (0.3597)\tPrec 92.188% (87.523%)\n",
      "Epoch: [83][200/391]\tTime 0.051 (0.051)\tData 0.002 (0.006)\tLoss 0.2932 (0.3659)\tPrec 89.844% (87.310%)\n",
      "Epoch: [83][300/391]\tTime 0.046 (0.050)\tData 0.002 (0.005)\tLoss 0.4174 (0.3676)\tPrec 85.156% (87.212%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.371 (0.371)\tLoss 0.5252 (0.5252)\tPrec 83.594% (83.594%)\n",
      " * Prec 80.950% \n",
      "best acc: 83.130000\n",
      "Epoch: [84][0/391]\tTime 0.729 (0.729)\tData 0.656 (0.656)\tLoss 0.4932 (0.4932)\tPrec 80.469% (80.469%)\n",
      "Epoch: [84][100/391]\tTime 0.056 (0.062)\tData 0.002 (0.008)\tLoss 0.3777 (0.3616)\tPrec 85.156% (87.252%)\n",
      "Epoch: [84][200/391]\tTime 0.043 (0.058)\tData 0.002 (0.005)\tLoss 0.2531 (0.3654)\tPrec 89.844% (87.197%)\n",
      "Epoch: [84][300/391]\tTime 0.054 (0.057)\tData 0.002 (0.004)\tLoss 0.4005 (0.3679)\tPrec 89.062% (87.100%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.598 (0.598)\tLoss 0.5380 (0.5380)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.210% \n",
      "best acc: 83.130000\n",
      "Epoch: [85][0/391]\tTime 0.762 (0.762)\tData 0.687 (0.687)\tLoss 0.3398 (0.3398)\tPrec 86.719% (86.719%)\n",
      "Epoch: [85][100/391]\tTime 0.040 (0.049)\tData 0.002 (0.008)\tLoss 0.4267 (0.3655)\tPrec 85.156% (87.098%)\n",
      "Epoch: [85][200/391]\tTime 0.061 (0.051)\tData 0.003 (0.005)\tLoss 0.4236 (0.3645)\tPrec 84.375% (87.150%)\n",
      "Epoch: [85][300/391]\tTime 0.051 (0.051)\tData 0.003 (0.004)\tLoss 0.3066 (0.3620)\tPrec 89.062% (87.324%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.291 (0.291)\tLoss 0.5333 (0.5333)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.590% \n",
      "best acc: 83.590000\n",
      "Epoch: [86][0/391]\tTime 0.784 (0.784)\tData 0.709 (0.709)\tLoss 0.2542 (0.2542)\tPrec 88.281% (88.281%)\n",
      "Epoch: [86][100/391]\tTime 0.040 (0.053)\tData 0.001 (0.009)\tLoss 0.4384 (0.3539)\tPrec 85.156% (87.817%)\n",
      "Epoch: [86][200/391]\tTime 0.039 (0.046)\tData 0.002 (0.005)\tLoss 0.3616 (0.3595)\tPrec 85.938% (87.418%)\n",
      "Epoch: [86][300/391]\tTime 0.040 (0.044)\tData 0.001 (0.004)\tLoss 0.3910 (0.3586)\tPrec 83.594% (87.440%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.265 (0.265)\tLoss 0.6901 (0.6901)\tPrec 77.344% (77.344%)\n",
      " * Prec 79.820% \n",
      "best acc: 83.590000\n",
      "Epoch: [87][0/391]\tTime 0.577 (0.577)\tData 0.504 (0.504)\tLoss 0.3589 (0.3589)\tPrec 84.375% (84.375%)\n",
      "Epoch: [87][100/391]\tTime 0.050 (0.056)\tData 0.002 (0.007)\tLoss 0.3241 (0.3562)\tPrec 89.062% (87.438%)\n",
      "Epoch: [87][200/391]\tTime 0.048 (0.054)\tData 0.003 (0.005)\tLoss 0.3816 (0.3512)\tPrec 88.281% (87.527%)\n",
      "Epoch: [87][300/391]\tTime 0.045 (0.054)\tData 0.002 (0.004)\tLoss 0.3114 (0.3562)\tPrec 89.844% (87.544%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.297 (0.297)\tLoss 0.5416 (0.5416)\tPrec 80.469% (80.469%)\n",
      " * Prec 82.700% \n",
      "best acc: 83.590000\n",
      "Epoch: [88][0/391]\tTime 0.836 (0.836)\tData 0.761 (0.761)\tLoss 0.4056 (0.4056)\tPrec 86.719% (86.719%)\n",
      "Epoch: [88][100/391]\tTime 0.056 (0.061)\tData 0.003 (0.010)\tLoss 0.2637 (0.3486)\tPrec 91.406% (87.871%)\n",
      "Epoch: [88][200/391]\tTime 0.050 (0.057)\tData 0.002 (0.006)\tLoss 0.3045 (0.3463)\tPrec 89.062% (87.963%)\n",
      "Epoch: [88][300/391]\tTime 0.053 (0.056)\tData 0.002 (0.005)\tLoss 0.3482 (0.3480)\tPrec 86.719% (87.936%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 0.6274 (0.6274)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.670% \n",
      "best acc: 83.590000\n",
      "Epoch: [89][0/391]\tTime 0.669 (0.669)\tData 0.597 (0.597)\tLoss 0.3241 (0.3241)\tPrec 88.281% (88.281%)\n",
      "Epoch: [89][100/391]\tTime 0.061 (0.060)\tData 0.003 (0.008)\tLoss 0.5105 (0.3505)\tPrec 83.594% (88.096%)\n",
      "Epoch: [89][200/391]\tTime 0.060 (0.058)\tData 0.003 (0.005)\tLoss 0.3122 (0.3520)\tPrec 87.500% (87.830%)\n",
      "Epoch: [89][300/391]\tTime 0.053 (0.056)\tData 0.002 (0.004)\tLoss 0.2579 (0.3527)\tPrec 89.062% (87.728%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.517 (0.517)\tLoss 0.5350 (0.5350)\tPrec 82.031% (82.031%)\n",
      " * Prec 82.240% \n",
      "best acc: 83.590000\n",
      "Epoch: [90][0/391]\tTime 0.626 (0.626)\tData 0.551 (0.551)\tLoss 0.2742 (0.2742)\tPrec 89.844% (89.844%)\n",
      "Epoch: [90][100/391]\tTime 0.048 (0.055)\tData 0.002 (0.008)\tLoss 0.3818 (0.3543)\tPrec 86.719% (87.763%)\n",
      "Epoch: [90][200/391]\tTime 0.052 (0.053)\tData 0.002 (0.005)\tLoss 0.3567 (0.3533)\tPrec 88.281% (87.632%)\n",
      "Epoch: [90][300/391]\tTime 0.056 (0.052)\tData 0.002 (0.004)\tLoss 0.3256 (0.3532)\tPrec 85.156% (87.627%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.423 (0.423)\tLoss 0.4537 (0.4537)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.520% \n",
      "best acc: 83.590000\n",
      "Epoch: [91][0/391]\tTime 0.623 (0.623)\tData 0.550 (0.550)\tLoss 0.3195 (0.3195)\tPrec 87.500% (87.500%)\n",
      "Epoch: [91][100/391]\tTime 0.050 (0.056)\tData 0.002 (0.008)\tLoss 0.3826 (0.3434)\tPrec 84.375% (87.794%)\n",
      "Epoch: [91][200/391]\tTime 0.051 (0.055)\tData 0.002 (0.005)\tLoss 0.4474 (0.3430)\tPrec 86.719% (87.939%)\n",
      "Epoch: [91][300/391]\tTime 0.053 (0.054)\tData 0.002 (0.004)\tLoss 0.5018 (0.3432)\tPrec 82.031% (87.939%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.295 (0.295)\tLoss 0.5728 (0.5728)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.610% \n",
      "best acc: 83.590000\n",
      "Epoch: [92][0/391]\tTime 1.048 (1.048)\tData 0.966 (0.966)\tLoss 0.2516 (0.2516)\tPrec 91.406% (91.406%)\n",
      "Epoch: [92][100/391]\tTime 0.066 (0.065)\tData 0.003 (0.012)\tLoss 0.3431 (0.3430)\tPrec 87.500% (88.258%)\n",
      "Epoch: [92][200/391]\tTime 0.060 (0.059)\tData 0.003 (0.007)\tLoss 0.2029 (0.3435)\tPrec 93.750% (87.990%)\n",
      "Epoch: [92][300/391]\tTime 0.060 (0.058)\tData 0.002 (0.005)\tLoss 0.2855 (0.3454)\tPrec 92.969% (88.017%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.443 (0.443)\tLoss 0.5658 (0.5658)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.310% \n",
      "best acc: 83.590000\n",
      "Epoch: [93][0/391]\tTime 0.618 (0.618)\tData 0.519 (0.519)\tLoss 0.3305 (0.3305)\tPrec 87.500% (87.500%)\n",
      "Epoch: [93][100/391]\tTime 0.044 (0.058)\tData 0.002 (0.007)\tLoss 0.4324 (0.3429)\tPrec 83.594% (87.987%)\n",
      "Epoch: [93][200/391]\tTime 0.045 (0.054)\tData 0.002 (0.005)\tLoss 0.4012 (0.3419)\tPrec 83.594% (87.966%)\n",
      "Epoch: [93][300/391]\tTime 0.045 (0.052)\tData 0.002 (0.004)\tLoss 0.4019 (0.3445)\tPrec 88.281% (87.996%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.334 (0.334)\tLoss 0.5485 (0.5485)\tPrec 82.812% (82.812%)\n",
      " * Prec 82.880% \n",
      "best acc: 83.590000\n",
      "Epoch: [94][0/391]\tTime 0.666 (0.666)\tData 0.591 (0.591)\tLoss 0.3472 (0.3472)\tPrec 87.500% (87.500%)\n",
      "Epoch: [94][100/391]\tTime 0.058 (0.060)\tData 0.002 (0.008)\tLoss 0.4275 (0.3422)\tPrec 87.500% (87.887%)\n",
      "Epoch: [94][200/391]\tTime 0.047 (0.056)\tData 0.002 (0.005)\tLoss 0.2921 (0.3410)\tPrec 87.500% (87.858%)\n",
      "Epoch: [94][300/391]\tTime 0.049 (0.053)\tData 0.002 (0.004)\tLoss 0.3990 (0.3394)\tPrec 87.500% (87.928%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.279 (0.279)\tLoss 0.5459 (0.5459)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.470% \n",
      "best acc: 83.590000\n",
      "Epoch: [95][0/391]\tTime 0.709 (0.709)\tData 0.634 (0.634)\tLoss 0.2787 (0.2787)\tPrec 92.188% (92.188%)\n",
      "Epoch: [95][100/391]\tTime 0.053 (0.058)\tData 0.002 (0.008)\tLoss 0.3855 (0.3256)\tPrec 85.156% (88.637%)\n",
      "Epoch: [95][200/391]\tTime 0.053 (0.054)\tData 0.002 (0.005)\tLoss 0.2216 (0.3280)\tPrec 90.625% (88.413%)\n",
      "Epoch: [95][300/391]\tTime 0.045 (0.052)\tData 0.002 (0.004)\tLoss 0.3996 (0.3354)\tPrec 85.938% (88.203%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.377 (0.377)\tLoss 0.5530 (0.5530)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.360% \n",
      "best acc: 83.590000\n",
      "Epoch: [96][0/391]\tTime 0.472 (0.472)\tData 0.401 (0.401)\tLoss 0.3158 (0.3158)\tPrec 89.844% (89.844%)\n",
      "Epoch: [96][100/391]\tTime 0.049 (0.058)\tData 0.002 (0.006)\tLoss 0.4942 (0.3317)\tPrec 81.250% (88.142%)\n",
      "Epoch: [96][200/391]\tTime 0.050 (0.056)\tData 0.002 (0.004)\tLoss 0.4114 (0.3292)\tPrec 83.594% (88.340%)\n",
      "Epoch: [96][300/391]\tTime 0.051 (0.055)\tData 0.002 (0.003)\tLoss 0.2267 (0.3351)\tPrec 89.062% (88.222%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.405 (0.405)\tLoss 0.5301 (0.5301)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.340% \n",
      "best acc: 84.340000\n",
      "Epoch: [97][0/391]\tTime 0.658 (0.658)\tData 0.585 (0.585)\tLoss 0.3406 (0.3406)\tPrec 88.281% (88.281%)\n",
      "Epoch: [97][100/391]\tTime 0.056 (0.062)\tData 0.003 (0.008)\tLoss 0.4979 (0.3267)\tPrec 83.594% (88.676%)\n",
      "Epoch: [97][200/391]\tTime 0.059 (0.060)\tData 0.002 (0.005)\tLoss 0.3499 (0.3266)\tPrec 86.719% (88.639%)\n",
      "Epoch: [97][300/391]\tTime 0.044 (0.059)\tData 0.002 (0.004)\tLoss 0.3445 (0.3311)\tPrec 86.719% (88.336%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.378 (0.378)\tLoss 0.4977 (0.4977)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.920% \n",
      "best acc: 84.340000\n",
      "Epoch: [98][0/391]\tTime 0.629 (0.629)\tData 0.559 (0.559)\tLoss 0.1552 (0.1552)\tPrec 96.094% (96.094%)\n",
      "Epoch: [98][100/391]\tTime 0.060 (0.063)\tData 0.002 (0.008)\tLoss 0.2493 (0.3372)\tPrec 88.281% (88.049%)\n",
      "Epoch: [98][200/391]\tTime 0.057 (0.060)\tData 0.002 (0.005)\tLoss 0.3811 (0.3355)\tPrec 85.938% (88.211%)\n",
      "Epoch: [98][300/391]\tTime 0.046 (0.058)\tData 0.002 (0.004)\tLoss 0.4770 (0.3373)\tPrec 84.375% (88.175%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 0.5077 (0.5077)\tPrec 82.031% (82.031%)\n",
      " * Prec 83.200% \n",
      "best acc: 84.340000\n",
      "Epoch: [99][0/391]\tTime 0.656 (0.656)\tData 0.586 (0.586)\tLoss 0.2849 (0.2849)\tPrec 91.406% (91.406%)\n",
      "Epoch: [99][100/391]\tTime 0.045 (0.060)\tData 0.002 (0.008)\tLoss 0.3147 (0.3148)\tPrec 89.844% (89.132%)\n",
      "Epoch: [99][200/391]\tTime 0.044 (0.056)\tData 0.002 (0.005)\tLoss 0.3774 (0.3187)\tPrec 85.156% (88.771%)\n",
      "Epoch: [99][300/391]\tTime 0.063 (0.055)\tData 0.003 (0.004)\tLoss 0.2214 (0.3221)\tPrec 92.188% (88.681%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.325 (0.325)\tLoss 0.5041 (0.5041)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.830% \n",
      "best acc: 84.340000\n",
      "Epoch: [100][0/391]\tTime 0.707 (0.707)\tData 0.639 (0.639)\tLoss 0.3097 (0.3097)\tPrec 90.625% (90.625%)\n",
      "Epoch: [100][100/391]\tTime 0.050 (0.063)\tData 0.003 (0.009)\tLoss 0.3904 (0.3291)\tPrec 87.500% (88.188%)\n",
      "Epoch: [100][200/391]\tTime 0.045 (0.058)\tData 0.002 (0.006)\tLoss 0.2807 (0.3239)\tPrec 90.625% (88.398%)\n",
      "Epoch: [100][300/391]\tTime 0.061 (0.057)\tData 0.002 (0.004)\tLoss 0.2839 (0.3240)\tPrec 90.625% (88.486%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.433 (0.433)\tLoss 0.6935 (0.6935)\tPrec 82.812% (82.812%)\n",
      " * Prec 81.780% \n",
      "best acc: 84.340000\n",
      "Epoch: [101][0/391]\tTime 0.713 (0.713)\tData 0.643 (0.643)\tLoss 0.3272 (0.3272)\tPrec 86.719% (86.719%)\n",
      "Epoch: [101][100/391]\tTime 0.049 (0.059)\tData 0.002 (0.009)\tLoss 0.3557 (0.3146)\tPrec 87.500% (88.745%)\n",
      "Epoch: [101][200/391]\tTime 0.048 (0.056)\tData 0.002 (0.005)\tLoss 0.3688 (0.3192)\tPrec 86.719% (88.697%)\n",
      "Epoch: [101][300/391]\tTime 0.052 (0.054)\tData 0.002 (0.004)\tLoss 0.3398 (0.3260)\tPrec 85.938% (88.458%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.273 (0.273)\tLoss 0.4817 (0.4817)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.120% \n",
      "best acc: 84.340000\n",
      "Epoch: [102][0/391]\tTime 0.739 (0.739)\tData 0.671 (0.671)\tLoss 0.3842 (0.3842)\tPrec 85.156% (85.156%)\n",
      "Epoch: [102][100/391]\tTime 0.059 (0.063)\tData 0.003 (0.009)\tLoss 0.2602 (0.3211)\tPrec 91.406% (88.459%)\n",
      "Epoch: [102][200/391]\tTime 0.044 (0.059)\tData 0.002 (0.006)\tLoss 0.2311 (0.3159)\tPrec 92.188% (88.802%)\n",
      "Epoch: [102][300/391]\tTime 0.047 (0.057)\tData 0.002 (0.005)\tLoss 0.3159 (0.3189)\tPrec 88.281% (88.652%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.473 (0.473)\tLoss 0.5774 (0.5774)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.230% \n",
      "best acc: 84.340000\n",
      "Epoch: [103][0/391]\tTime 0.616 (0.616)\tData 0.542 (0.542)\tLoss 0.3064 (0.3064)\tPrec 89.844% (89.844%)\n",
      "Epoch: [103][100/391]\tTime 0.060 (0.063)\tData 0.002 (0.008)\tLoss 0.3281 (0.3083)\tPrec 85.938% (89.163%)\n",
      "Epoch: [103][200/391]\tTime 0.056 (0.058)\tData 0.002 (0.005)\tLoss 0.3694 (0.3173)\tPrec 84.375% (88.783%)\n",
      "Epoch: [103][300/391]\tTime 0.060 (0.057)\tData 0.002 (0.004)\tLoss 0.4498 (0.3197)\tPrec 84.375% (88.704%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.528 (0.528)\tLoss 0.5734 (0.5734)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.680% \n",
      "best acc: 84.340000\n",
      "Epoch: [104][0/391]\tTime 0.655 (0.655)\tData 0.580 (0.580)\tLoss 0.3143 (0.3143)\tPrec 90.625% (90.625%)\n",
      "Epoch: [104][100/391]\tTime 0.055 (0.060)\tData 0.003 (0.008)\tLoss 0.3371 (0.3105)\tPrec 89.844% (89.148%)\n",
      "Epoch: [104][200/391]\tTime 0.044 (0.056)\tData 0.002 (0.005)\tLoss 0.2703 (0.3139)\tPrec 89.844% (89.020%)\n",
      "Epoch: [104][300/391]\tTime 0.055 (0.055)\tData 0.002 (0.004)\tLoss 0.2439 (0.3177)\tPrec 90.625% (88.855%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.295 (0.295)\tLoss 0.6234 (0.6234)\tPrec 85.938% (85.938%)\n",
      " * Prec 81.650% \n",
      "best acc: 84.340000\n",
      "Epoch: [105][0/391]\tTime 0.595 (0.595)\tData 0.522 (0.522)\tLoss 0.2749 (0.2749)\tPrec 88.281% (88.281%)\n",
      "Epoch: [105][100/391]\tTime 0.048 (0.060)\tData 0.003 (0.008)\tLoss 0.3194 (0.3212)\tPrec 88.281% (88.653%)\n",
      "Epoch: [105][200/391]\tTime 0.051 (0.057)\tData 0.002 (0.005)\tLoss 0.4051 (0.3179)\tPrec 87.500% (88.884%)\n",
      "Epoch: [105][300/391]\tTime 0.049 (0.055)\tData 0.002 (0.004)\tLoss 0.2807 (0.3172)\tPrec 90.625% (88.870%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.5057 (0.5057)\tPrec 81.250% (81.250%)\n",
      " * Prec 83.460% \n",
      "best acc: 84.340000\n",
      "Epoch: [106][0/391]\tTime 0.739 (0.739)\tData 0.662 (0.662)\tLoss 0.3495 (0.3495)\tPrec 84.375% (84.375%)\n",
      "Epoch: [106][100/391]\tTime 0.055 (0.060)\tData 0.002 (0.009)\tLoss 0.2585 (0.3077)\tPrec 90.625% (89.070%)\n",
      "Epoch: [106][200/391]\tTime 0.042 (0.056)\tData 0.002 (0.006)\tLoss 0.3221 (0.3092)\tPrec 86.719% (89.132%)\n",
      "Epoch: [106][300/391]\tTime 0.054 (0.055)\tData 0.002 (0.004)\tLoss 0.3051 (0.3121)\tPrec 88.281% (88.951%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.555 (0.555)\tLoss 0.4502 (0.4502)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.720% \n",
      "best acc: 84.340000\n",
      "Epoch: [107][0/391]\tTime 0.665 (0.665)\tData 0.591 (0.591)\tLoss 0.2841 (0.2841)\tPrec 89.062% (89.062%)\n",
      "Epoch: [107][100/391]\tTime 0.052 (0.058)\tData 0.002 (0.008)\tLoss 0.3090 (0.3052)\tPrec 89.844% (89.093%)\n",
      "Epoch: [107][200/391]\tTime 0.067 (0.056)\tData 0.003 (0.005)\tLoss 0.2413 (0.3098)\tPrec 93.750% (88.985%)\n",
      "Epoch: [107][300/391]\tTime 0.046 (0.055)\tData 0.002 (0.004)\tLoss 0.4197 (0.3115)\tPrec 87.500% (89.039%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.307 (0.307)\tLoss 0.5743 (0.5743)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.190% \n",
      "best acc: 84.340000\n",
      "Epoch: [108][0/391]\tTime 0.866 (0.866)\tData 0.793 (0.793)\tLoss 0.4067 (0.4067)\tPrec 88.281% (88.281%)\n",
      "Epoch: [108][100/391]\tTime 0.054 (0.060)\tData 0.002 (0.010)\tLoss 0.2377 (0.3089)\tPrec 89.844% (89.179%)\n",
      "Epoch: [108][200/391]\tTime 0.067 (0.056)\tData 0.003 (0.006)\tLoss 0.5135 (0.3096)\tPrec 86.719% (89.043%)\n",
      "Epoch: [108][300/391]\tTime 0.052 (0.056)\tData 0.002 (0.005)\tLoss 0.2509 (0.3130)\tPrec 90.625% (88.982%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.368 (0.368)\tLoss 0.5170 (0.5170)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.370% \n",
      "best acc: 84.370000\n",
      "Epoch: [109][0/391]\tTime 0.714 (0.714)\tData 0.637 (0.637)\tLoss 0.3324 (0.3324)\tPrec 89.062% (89.062%)\n",
      "Epoch: [109][100/391]\tTime 0.049 (0.060)\tData 0.002 (0.008)\tLoss 0.2550 (0.3132)\tPrec 90.625% (88.745%)\n",
      "Epoch: [109][200/391]\tTime 0.051 (0.056)\tData 0.002 (0.005)\tLoss 0.3247 (0.3126)\tPrec 87.500% (88.891%)\n",
      "Epoch: [109][300/391]\tTime 0.046 (0.054)\tData 0.002 (0.004)\tLoss 0.3636 (0.3106)\tPrec 89.844% (88.964%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.472 (0.472)\tLoss 0.6080 (0.6080)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.010% \n",
      "best acc: 84.370000\n",
      "Epoch: [110][0/391]\tTime 0.845 (0.845)\tData 0.776 (0.776)\tLoss 0.3477 (0.3477)\tPrec 86.719% (86.719%)\n",
      "Epoch: [110][100/391]\tTime 0.047 (0.063)\tData 0.002 (0.010)\tLoss 0.3365 (0.3076)\tPrec 85.938% (89.117%)\n",
      "Epoch: [110][200/391]\tTime 0.054 (0.059)\tData 0.002 (0.006)\tLoss 0.2307 (0.3054)\tPrec 92.969% (89.249%)\n",
      "Epoch: [110][300/391]\tTime 0.052 (0.057)\tData 0.002 (0.005)\tLoss 0.3892 (0.3094)\tPrec 87.500% (89.148%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.5564 (0.5564)\tPrec 80.469% (80.469%)\n",
      " * Prec 82.440% \n",
      "best acc: 84.370000\n",
      "Epoch: [111][0/391]\tTime 0.682 (0.682)\tData 0.609 (0.609)\tLoss 0.2553 (0.2553)\tPrec 89.844% (89.844%)\n",
      "Epoch: [111][100/391]\tTime 0.059 (0.063)\tData 0.002 (0.008)\tLoss 0.3582 (0.3029)\tPrec 84.375% (89.279%)\n",
      "Epoch: [111][200/391]\tTime 0.063 (0.057)\tData 0.004 (0.005)\tLoss 0.3683 (0.3032)\tPrec 87.500% (89.226%)\n",
      "Epoch: [111][300/391]\tTime 0.057 (0.058)\tData 0.002 (0.004)\tLoss 0.3733 (0.3059)\tPrec 85.938% (89.208%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.401 (0.401)\tLoss 0.6764 (0.6764)\tPrec 75.781% (75.781%)\n",
      " * Prec 81.670% \n",
      "best acc: 84.370000\n",
      "Epoch: [112][0/391]\tTime 0.612 (0.612)\tData 0.540 (0.540)\tLoss 0.2547 (0.2547)\tPrec 89.062% (89.062%)\n",
      "Epoch: [112][100/391]\tTime 0.062 (0.064)\tData 0.002 (0.008)\tLoss 0.2569 (0.3041)\tPrec 92.188% (89.619%)\n",
      "Epoch: [112][200/391]\tTime 0.056 (0.060)\tData 0.002 (0.005)\tLoss 0.2257 (0.2985)\tPrec 94.531% (89.805%)\n",
      "Epoch: [112][300/391]\tTime 0.050 (0.057)\tData 0.002 (0.004)\tLoss 0.2138 (0.3043)\tPrec 89.062% (89.460%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.299 (0.299)\tLoss 0.6020 (0.6020)\tPrec 79.688% (79.688%)\n",
      " * Prec 82.800% \n",
      "best acc: 84.370000\n",
      "Epoch: [113][0/391]\tTime 0.672 (0.672)\tData 0.599 (0.599)\tLoss 0.2727 (0.2727)\tPrec 89.844% (89.844%)\n",
      "Epoch: [113][100/391]\tTime 0.061 (0.057)\tData 0.003 (0.008)\tLoss 0.2678 (0.2978)\tPrec 92.188% (89.465%)\n",
      "Epoch: [113][200/391]\tTime 0.061 (0.053)\tData 0.003 (0.005)\tLoss 0.2374 (0.2970)\tPrec 94.531% (89.607%)\n",
      "Epoch: [113][300/391]\tTime 0.054 (0.052)\tData 0.003 (0.004)\tLoss 0.3041 (0.2988)\tPrec 88.281% (89.610%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.259 (0.259)\tLoss 0.5839 (0.5839)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.950% \n",
      "best acc: 84.950000\n",
      "Epoch: [114][0/391]\tTime 0.644 (0.644)\tData 0.571 (0.571)\tLoss 0.3153 (0.3153)\tPrec 87.500% (87.500%)\n",
      "Epoch: [114][100/391]\tTime 0.057 (0.060)\tData 0.002 (0.008)\tLoss 0.3260 (0.2837)\tPrec 89.062% (90.207%)\n",
      "Epoch: [114][200/391]\tTime 0.049 (0.058)\tData 0.002 (0.005)\tLoss 0.2696 (0.2954)\tPrec 90.625% (89.770%)\n",
      "Epoch: [114][300/391]\tTime 0.045 (0.057)\tData 0.002 (0.004)\tLoss 0.2522 (0.2984)\tPrec 93.750% (89.685%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.5857 (0.5857)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.490% \n",
      "best acc: 84.950000\n",
      "Epoch: [115][0/391]\tTime 0.612 (0.612)\tData 0.543 (0.543)\tLoss 0.2486 (0.2486)\tPrec 90.625% (90.625%)\n",
      "Epoch: [115][100/391]\tTime 0.060 (0.064)\tData 0.003 (0.009)\tLoss 0.1870 (0.2927)\tPrec 92.969% (89.488%)\n",
      "Epoch: [115][200/391]\tTime 0.045 (0.060)\tData 0.002 (0.006)\tLoss 0.2768 (0.2906)\tPrec 88.281% (89.661%)\n",
      "Epoch: [115][300/391]\tTime 0.059 (0.058)\tData 0.003 (0.005)\tLoss 0.3915 (0.2949)\tPrec 83.594% (89.530%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 0.5859 (0.5859)\tPrec 80.469% (80.469%)\n",
      " * Prec 83.920% \n",
      "best acc: 84.950000\n",
      "Epoch: [116][0/391]\tTime 0.665 (0.665)\tData 0.592 (0.592)\tLoss 0.2315 (0.2315)\tPrec 91.406% (91.406%)\n",
      "Epoch: [116][100/391]\tTime 0.045 (0.059)\tData 0.002 (0.008)\tLoss 0.3370 (0.3010)\tPrec 87.500% (89.233%)\n",
      "Epoch: [116][200/391]\tTime 0.052 (0.057)\tData 0.002 (0.005)\tLoss 0.3071 (0.3005)\tPrec 89.844% (89.370%)\n",
      "Epoch: [116][300/391]\tTime 0.060 (0.057)\tData 0.002 (0.004)\tLoss 0.3679 (0.3004)\tPrec 90.625% (89.452%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.295 (0.295)\tLoss 0.6697 (0.6697)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.570% \n",
      "best acc: 84.950000\n",
      "Epoch: [117][0/391]\tTime 0.735 (0.735)\tData 0.668 (0.668)\tLoss 0.2710 (0.2710)\tPrec 91.406% (91.406%)\n",
      "Epoch: [117][100/391]\tTime 0.042 (0.065)\tData 0.001 (0.009)\tLoss 0.2981 (0.3002)\tPrec 86.719% (89.434%)\n",
      "Epoch: [117][200/391]\tTime 0.058 (0.059)\tData 0.002 (0.006)\tLoss 0.3215 (0.2995)\tPrec 87.500% (89.432%)\n",
      "Epoch: [117][300/391]\tTime 0.047 (0.057)\tData 0.002 (0.005)\tLoss 0.2807 (0.2994)\tPrec 93.750% (89.444%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.4569 (0.4569)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.070% \n",
      "best acc: 84.950000\n",
      "Epoch: [118][0/391]\tTime 0.632 (0.632)\tData 0.560 (0.560)\tLoss 0.2924 (0.2924)\tPrec 88.281% (88.281%)\n",
      "Epoch: [118][100/391]\tTime 0.053 (0.062)\tData 0.002 (0.008)\tLoss 0.3287 (0.2919)\tPrec 86.719% (89.643%)\n",
      "Epoch: [118][200/391]\tTime 0.062 (0.059)\tData 0.003 (0.005)\tLoss 0.2262 (0.2963)\tPrec 93.750% (89.544%)\n",
      "Epoch: [118][300/391]\tTime 0.054 (0.058)\tData 0.002 (0.004)\tLoss 0.2292 (0.2977)\tPrec 91.406% (89.488%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.5222 (0.5222)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.020% \n",
      "best acc: 85.020000\n",
      "Epoch: [119][0/391]\tTime 0.697 (0.697)\tData 0.624 (0.624)\tLoss 0.3297 (0.3297)\tPrec 87.500% (87.500%)\n",
      "Epoch: [119][100/391]\tTime 0.045 (0.057)\tData 0.002 (0.008)\tLoss 0.3165 (0.2914)\tPrec 87.500% (89.998%)\n",
      "Epoch: [119][200/391]\tTime 0.059 (0.055)\tData 0.002 (0.005)\tLoss 0.3311 (0.2941)\tPrec 87.500% (89.813%)\n",
      "Epoch: [119][300/391]\tTime 0.045 (0.054)\tData 0.002 (0.004)\tLoss 0.2350 (0.2969)\tPrec 90.625% (89.670%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.4997 (0.4997)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.300% \n",
      "best acc: 85.020000\n",
      "Epoch: [120][0/391]\tTime 0.530 (0.530)\tData 0.459 (0.459)\tLoss 0.3494 (0.3494)\tPrec 89.844% (89.844%)\n",
      "Epoch: [120][100/391]\tTime 0.051 (0.058)\tData 0.003 (0.007)\tLoss 0.3110 (0.2887)\tPrec 89.062% (89.906%)\n",
      "Epoch: [120][200/391]\tTime 0.057 (0.056)\tData 0.003 (0.005)\tLoss 0.2195 (0.2929)\tPrec 92.188% (89.758%)\n",
      "Epoch: [120][300/391]\tTime 0.060 (0.057)\tData 0.003 (0.004)\tLoss 0.2752 (0.2951)\tPrec 88.281% (89.641%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.315 (0.315)\tLoss 0.5312 (0.5312)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.430% \n",
      "best acc: 85.020000\n",
      "Epoch: [121][0/391]\tTime 0.595 (0.595)\tData 0.520 (0.520)\tLoss 0.3697 (0.3697)\tPrec 87.500% (87.500%)\n",
      "Epoch: [121][100/391]\tTime 0.068 (0.062)\tData 0.002 (0.007)\tLoss 0.2301 (0.2807)\tPrec 91.406% (89.859%)\n",
      "Epoch: [121][200/391]\tTime 0.046 (0.058)\tData 0.002 (0.005)\tLoss 0.3025 (0.2902)\tPrec 89.844% (89.731%)\n",
      "Epoch: [121][300/391]\tTime 0.051 (0.056)\tData 0.002 (0.004)\tLoss 0.2625 (0.2905)\tPrec 92.188% (89.740%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.270 (0.270)\tLoss 0.5877 (0.5877)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.640% \n",
      "best acc: 85.020000\n",
      "Epoch: [122][0/391]\tTime 0.682 (0.682)\tData 0.588 (0.588)\tLoss 0.2113 (0.2113)\tPrec 92.188% (92.188%)\n",
      "Epoch: [122][100/391]\tTime 0.045 (0.057)\tData 0.002 (0.008)\tLoss 0.2426 (0.2773)\tPrec 92.969% (90.339%)\n",
      "Epoch: [122][200/391]\tTime 0.059 (0.055)\tData 0.002 (0.005)\tLoss 0.3579 (0.2821)\tPrec 84.375% (90.127%)\n",
      "Epoch: [122][300/391]\tTime 0.057 (0.055)\tData 0.003 (0.004)\tLoss 0.2951 (0.2883)\tPrec 91.406% (89.924%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.5370 (0.5370)\tPrec 82.031% (82.031%)\n",
      " * Prec 84.340% \n",
      "best acc: 85.020000\n",
      "Epoch: [123][0/391]\tTime 0.621 (0.621)\tData 0.548 (0.548)\tLoss 0.2627 (0.2627)\tPrec 90.625% (90.625%)\n",
      "Epoch: [123][100/391]\tTime 0.044 (0.060)\tData 0.002 (0.008)\tLoss 0.2597 (0.2805)\tPrec 92.188% (89.983%)\n",
      "Epoch: [123][200/391]\tTime 0.060 (0.057)\tData 0.003 (0.005)\tLoss 0.2133 (0.2827)\tPrec 89.062% (89.941%)\n",
      "Epoch: [123][300/391]\tTime 0.048 (0.055)\tData 0.002 (0.004)\tLoss 0.1749 (0.2846)\tPrec 93.750% (89.961%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.180 (0.180)\tLoss 0.6959 (0.6959)\tPrec 82.812% (82.812%)\n",
      " * Prec 82.590% \n",
      "best acc: 85.020000\n",
      "Epoch: [124][0/391]\tTime 0.755 (0.755)\tData 0.685 (0.685)\tLoss 0.2995 (0.2995)\tPrec 90.625% (90.625%)\n",
      "Epoch: [124][100/391]\tTime 0.061 (0.066)\tData 0.003 (0.009)\tLoss 0.2900 (0.2857)\tPrec 87.500% (89.813%)\n",
      "Epoch: [124][200/391]\tTime 0.046 (0.061)\tData 0.002 (0.006)\tLoss 0.2817 (0.2844)\tPrec 87.500% (89.879%)\n",
      "Epoch: [124][300/391]\tTime 0.050 (0.059)\tData 0.002 (0.004)\tLoss 0.3522 (0.2841)\tPrec 89.062% (89.865%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.340 (0.340)\tLoss 0.4362 (0.4362)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.730% \n",
      "best acc: 85.020000\n",
      "Epoch: [125][0/391]\tTime 0.621 (0.621)\tData 0.548 (0.548)\tLoss 0.2625 (0.2625)\tPrec 90.625% (90.625%)\n",
      "Epoch: [125][100/391]\tTime 0.056 (0.061)\tData 0.003 (0.008)\tLoss 0.3621 (0.2766)\tPrec 87.500% (90.354%)\n",
      "Epoch: [125][200/391]\tTime 0.063 (0.057)\tData 0.003 (0.005)\tLoss 0.3873 (0.2800)\tPrec 89.062% (90.162%)\n",
      "Epoch: [125][300/391]\tTime 0.057 (0.056)\tData 0.002 (0.004)\tLoss 0.2738 (0.2833)\tPrec 92.188% (90.036%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.330 (0.330)\tLoss 0.7492 (0.7492)\tPrec 78.906% (78.906%)\n",
      " * Prec 81.720% \n",
      "best acc: 85.020000\n",
      "Epoch: [126][0/391]\tTime 0.671 (0.671)\tData 0.579 (0.579)\tLoss 0.2889 (0.2889)\tPrec 87.500% (87.500%)\n",
      "Epoch: [126][100/391]\tTime 0.062 (0.062)\tData 0.003 (0.008)\tLoss 0.4044 (0.2740)\tPrec 83.594% (90.254%)\n",
      "Epoch: [126][200/391]\tTime 0.059 (0.060)\tData 0.002 (0.005)\tLoss 0.4409 (0.2811)\tPrec 83.594% (89.976%)\n",
      "Epoch: [126][300/391]\tTime 0.050 (0.059)\tData 0.002 (0.004)\tLoss 0.2800 (0.2863)\tPrec 87.500% (89.802%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.423 (0.423)\tLoss 0.6625 (0.6625)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.360% \n",
      "best acc: 85.020000\n",
      "Epoch: [127][0/391]\tTime 0.855 (0.855)\tData 0.783 (0.783)\tLoss 0.3627 (0.3627)\tPrec 85.156% (85.156%)\n",
      "Epoch: [127][100/391]\tTime 0.046 (0.066)\tData 0.002 (0.010)\tLoss 0.3422 (0.2726)\tPrec 86.719% (90.037%)\n",
      "Epoch: [127][200/391]\tTime 0.060 (0.060)\tData 0.002 (0.006)\tLoss 0.2569 (0.2784)\tPrec 91.406% (89.980%)\n",
      "Epoch: [127][300/391]\tTime 0.062 (0.058)\tData 0.003 (0.005)\tLoss 0.4067 (0.2787)\tPrec 86.719% (90.090%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.4608 (0.4608)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.150% \n",
      "best acc: 85.020000\n",
      "Epoch: [128][0/391]\tTime 0.798 (0.798)\tData 0.696 (0.696)\tLoss 0.3034 (0.3034)\tPrec 89.062% (89.062%)\n",
      "Epoch: [128][100/391]\tTime 0.057 (0.061)\tData 0.003 (0.009)\tLoss 0.3574 (0.2759)\tPrec 89.062% (90.424%)\n",
      "Epoch: [128][200/391]\tTime 0.069 (0.058)\tData 0.003 (0.006)\tLoss 0.2227 (0.2755)\tPrec 91.406% (90.368%)\n",
      "Epoch: [128][300/391]\tTime 0.057 (0.057)\tData 0.002 (0.005)\tLoss 0.3357 (0.2773)\tPrec 89.062% (90.277%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.417 (0.417)\tLoss 0.5256 (0.5256)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.820% \n",
      "best acc: 85.020000\n",
      "Epoch: [129][0/391]\tTime 0.604 (0.604)\tData 0.533 (0.533)\tLoss 0.2770 (0.2770)\tPrec 92.969% (92.969%)\n",
      "Epoch: [129][100/391]\tTime 0.055 (0.057)\tData 0.002 (0.007)\tLoss 0.2949 (0.2782)\tPrec 89.062% (90.130%)\n",
      "Epoch: [129][200/391]\tTime 0.047 (0.055)\tData 0.002 (0.005)\tLoss 0.3285 (0.2801)\tPrec 89.062% (89.964%)\n",
      "Epoch: [129][300/391]\tTime 0.045 (0.054)\tData 0.002 (0.004)\tLoss 0.2428 (0.2800)\tPrec 89.844% (90.049%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.295 (0.295)\tLoss 0.7026 (0.7026)\tPrec 80.469% (80.469%)\n",
      " * Prec 82.450% \n",
      "best acc: 85.020000\n",
      "Epoch: [130][0/391]\tTime 0.583 (0.583)\tData 0.509 (0.509)\tLoss 0.4012 (0.4012)\tPrec 85.156% (85.156%)\n",
      "Epoch: [130][100/391]\tTime 0.057 (0.057)\tData 0.004 (0.007)\tLoss 0.2446 (0.2670)\tPrec 89.844% (90.478%)\n",
      "Epoch: [130][200/391]\tTime 0.047 (0.056)\tData 0.002 (0.005)\tLoss 0.4004 (0.2713)\tPrec 85.156% (90.384%)\n",
      "Epoch: [130][300/391]\tTime 0.048 (0.055)\tData 0.002 (0.004)\tLoss 0.2921 (0.2789)\tPrec 89.062% (90.116%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.375 (0.375)\tLoss 0.5770 (0.5770)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.220% \n",
      "best acc: 85.020000\n",
      "Epoch: [131][0/391]\tTime 0.617 (0.617)\tData 0.536 (0.536)\tLoss 0.3234 (0.3234)\tPrec 90.625% (90.625%)\n",
      "Epoch: [131][100/391]\tTime 0.051 (0.060)\tData 0.002 (0.008)\tLoss 0.2672 (0.2649)\tPrec 90.625% (90.486%)\n",
      "Epoch: [131][200/391]\tTime 0.060 (0.057)\tData 0.003 (0.005)\tLoss 0.3306 (0.2671)\tPrec 86.719% (90.512%)\n",
      "Epoch: [131][300/391]\tTime 0.066 (0.056)\tData 0.003 (0.004)\tLoss 0.2324 (0.2693)\tPrec 89.844% (90.472%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.362 (0.362)\tLoss 0.5587 (0.5587)\tPrec 82.812% (82.812%)\n",
      " * Prec 84.300% \n",
      "best acc: 85.020000\n",
      "Epoch: [132][0/391]\tTime 1.066 (1.066)\tData 0.993 (0.993)\tLoss 0.2429 (0.2429)\tPrec 92.188% (92.188%)\n",
      "Epoch: [132][100/391]\tTime 0.055 (0.065)\tData 0.002 (0.012)\tLoss 0.1673 (0.2622)\tPrec 95.312% (90.818%)\n",
      "Epoch: [132][200/391]\tTime 0.049 (0.061)\tData 0.002 (0.007)\tLoss 0.3663 (0.2738)\tPrec 85.156% (90.201%)\n",
      "Epoch: [132][300/391]\tTime 0.054 (0.059)\tData 0.002 (0.006)\tLoss 0.3513 (0.2794)\tPrec 87.500% (89.955%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.375 (0.375)\tLoss 0.5749 (0.5749)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.870% \n",
      "best acc: 85.020000\n",
      "Epoch: [133][0/391]\tTime 0.616 (0.616)\tData 0.541 (0.541)\tLoss 0.2745 (0.2745)\tPrec 90.625% (90.625%)\n",
      "Epoch: [133][100/391]\tTime 0.064 (0.056)\tData 0.004 (0.007)\tLoss 0.2556 (0.2646)\tPrec 90.625% (90.501%)\n",
      "Epoch: [133][200/391]\tTime 0.058 (0.054)\tData 0.002 (0.005)\tLoss 0.2384 (0.2711)\tPrec 91.406% (90.326%)\n",
      "Epoch: [133][300/391]\tTime 0.047 (0.054)\tData 0.002 (0.004)\tLoss 0.2335 (0.2714)\tPrec 91.406% (90.293%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.320 (0.320)\tLoss 0.5267 (0.5267)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.550% \n",
      "best acc: 85.020000\n",
      "Epoch: [134][0/391]\tTime 0.603 (0.603)\tData 0.530 (0.530)\tLoss 0.2785 (0.2785)\tPrec 91.406% (91.406%)\n",
      "Epoch: [134][100/391]\tTime 0.044 (0.058)\tData 0.002 (0.007)\tLoss 0.3001 (0.2657)\tPrec 92.188% (90.749%)\n",
      "Epoch: [134][200/391]\tTime 0.060 (0.057)\tData 0.002 (0.005)\tLoss 0.3100 (0.2711)\tPrec 89.062% (90.505%)\n",
      "Epoch: [134][300/391]\tTime 0.053 (0.055)\tData 0.002 (0.004)\tLoss 0.3603 (0.2736)\tPrec 85.938% (90.311%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.312 (0.312)\tLoss 0.5774 (0.5774)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.900% \n",
      "best acc: 85.020000\n",
      "Epoch: [135][0/391]\tTime 0.753 (0.753)\tData 0.682 (0.682)\tLoss 0.1458 (0.1458)\tPrec 96.094% (96.094%)\n",
      "Epoch: [135][100/391]\tTime 0.049 (0.061)\tData 0.003 (0.009)\tLoss 0.3801 (0.2659)\tPrec 85.938% (90.524%)\n",
      "Epoch: [135][200/391]\tTime 0.052 (0.058)\tData 0.003 (0.006)\tLoss 0.2582 (0.2682)\tPrec 92.969% (90.466%)\n",
      "Epoch: [135][300/391]\tTime 0.055 (0.056)\tData 0.002 (0.005)\tLoss 0.3610 (0.2716)\tPrec 87.500% (90.272%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.4443 (0.4443)\tPrec 83.594% (83.594%)\n",
      " * Prec 85.100% \n",
      "best acc: 85.100000\n",
      "Epoch: [136][0/391]\tTime 0.672 (0.672)\tData 0.597 (0.597)\tLoss 0.2870 (0.2870)\tPrec 89.844% (89.844%)\n",
      "Epoch: [136][100/391]\tTime 0.059 (0.063)\tData 0.002 (0.008)\tLoss 0.2220 (0.2602)\tPrec 89.844% (90.671%)\n",
      "Epoch: [136][200/391]\tTime 0.052 (0.059)\tData 0.002 (0.005)\tLoss 0.3143 (0.2605)\tPrec 89.844% (90.660%)\n",
      "Epoch: [136][300/391]\tTime 0.053 (0.058)\tData 0.003 (0.004)\tLoss 0.3144 (0.2650)\tPrec 87.500% (90.532%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.6616 (0.6616)\tPrec 81.250% (81.250%)\n",
      " * Prec 83.320% \n",
      "best acc: 85.100000\n",
      "Epoch: [137][0/391]\tTime 0.572 (0.572)\tData 0.500 (0.500)\tLoss 0.1685 (0.1685)\tPrec 93.750% (93.750%)\n",
      "Epoch: [137][100/391]\tTime 0.081 (0.061)\tData 0.002 (0.007)\tLoss 0.2817 (0.2667)\tPrec 90.625% (90.579%)\n",
      "Epoch: [137][200/391]\tTime 0.048 (0.057)\tData 0.002 (0.005)\tLoss 0.2674 (0.2692)\tPrec 92.188% (90.516%)\n",
      "Epoch: [137][300/391]\tTime 0.058 (0.056)\tData 0.003 (0.004)\tLoss 0.2605 (0.2690)\tPrec 89.844% (90.555%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.261 (0.261)\tLoss 0.5035 (0.5035)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.820% \n",
      "best acc: 85.100000\n",
      "Epoch: [138][0/391]\tTime 0.706 (0.706)\tData 0.633 (0.633)\tLoss 0.2876 (0.2876)\tPrec 91.406% (91.406%)\n",
      "Epoch: [138][100/391]\tTime 0.047 (0.060)\tData 0.002 (0.009)\tLoss 0.2956 (0.2546)\tPrec 89.062% (91.081%)\n",
      "Epoch: [138][200/391]\tTime 0.064 (0.057)\tData 0.003 (0.005)\tLoss 0.3278 (0.2622)\tPrec 88.281% (90.660%)\n",
      "Epoch: [138][300/391]\tTime 0.060 (0.056)\tData 0.002 (0.004)\tLoss 0.2335 (0.2666)\tPrec 90.625% (90.542%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.556 (0.556)\tLoss 0.4545 (0.4545)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.970% \n",
      "best acc: 85.100000\n",
      "Epoch: [139][0/391]\tTime 0.768 (0.768)\tData 0.697 (0.697)\tLoss 0.2387 (0.2387)\tPrec 93.750% (93.750%)\n",
      "Epoch: [139][100/391]\tTime 0.062 (0.063)\tData 0.002 (0.009)\tLoss 0.2346 (0.2592)\tPrec 93.750% (90.958%)\n",
      "Epoch: [139][200/391]\tTime 0.050 (0.060)\tData 0.002 (0.006)\tLoss 0.4307 (0.2616)\tPrec 82.812% (90.664%)\n",
      "Epoch: [139][300/391]\tTime 0.052 (0.060)\tData 0.003 (0.005)\tLoss 0.1786 (0.2655)\tPrec 93.750% (90.516%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.4820 (0.4820)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.700% \n",
      "best acc: 85.100000\n",
      "Epoch: [140][0/391]\tTime 0.872 (0.872)\tData 0.801 (0.801)\tLoss 0.1436 (0.1436)\tPrec 95.312% (95.312%)\n",
      "Epoch: [140][100/391]\tTime 0.048 (0.069)\tData 0.003 (0.010)\tLoss 0.2905 (0.2543)\tPrec 92.188% (90.857%)\n",
      "Epoch: [140][200/391]\tTime 0.058 (0.062)\tData 0.002 (0.006)\tLoss 0.2871 (0.2589)\tPrec 92.969% (90.757%)\n",
      "Epoch: [140][300/391]\tTime 0.051 (0.059)\tData 0.002 (0.005)\tLoss 0.2195 (0.2633)\tPrec 93.750% (90.646%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.405 (0.405)\tLoss 0.5066 (0.5066)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.640% \n",
      "best acc: 85.100000\n",
      "Epoch: [141][0/391]\tTime 0.623 (0.623)\tData 0.550 (0.550)\tLoss 0.2712 (0.2712)\tPrec 87.500% (87.500%)\n",
      "Epoch: [141][100/391]\tTime 0.049 (0.060)\tData 0.002 (0.008)\tLoss 0.3288 (0.2530)\tPrec 85.938% (91.058%)\n",
      "Epoch: [141][200/391]\tTime 0.058 (0.057)\tData 0.003 (0.005)\tLoss 0.2355 (0.2545)\tPrec 93.750% (90.885%)\n",
      "Epoch: [141][300/391]\tTime 0.061 (0.056)\tData 0.003 (0.004)\tLoss 0.3805 (0.2575)\tPrec 86.719% (90.822%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.306 (0.306)\tLoss 0.4693 (0.4693)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.520% \n",
      "best acc: 85.100000\n",
      "Epoch: [142][0/391]\tTime 0.650 (0.650)\tData 0.573 (0.573)\tLoss 0.2061 (0.2061)\tPrec 92.969% (92.969%)\n",
      "Epoch: [142][100/391]\tTime 0.062 (0.058)\tData 0.003 (0.008)\tLoss 0.2900 (0.2554)\tPrec 89.062% (90.656%)\n",
      "Epoch: [142][200/391]\tTime 0.051 (0.055)\tData 0.003 (0.005)\tLoss 0.2106 (0.2588)\tPrec 92.188% (90.625%)\n",
      "Epoch: [142][300/391]\tTime 0.048 (0.054)\tData 0.002 (0.004)\tLoss 0.2745 (0.2641)\tPrec 90.625% (90.446%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.408 (0.408)\tLoss 0.4560 (0.4560)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.390% \n",
      "best acc: 85.100000\n",
      "Epoch: [143][0/391]\tTime 0.582 (0.582)\tData 0.512 (0.512)\tLoss 0.2515 (0.2515)\tPrec 89.844% (89.844%)\n",
      "Epoch: [143][100/391]\tTime 0.048 (0.055)\tData 0.002 (0.007)\tLoss 0.1862 (0.2520)\tPrec 92.969% (91.081%)\n",
      "Epoch: [143][200/391]\tTime 0.042 (0.053)\tData 0.002 (0.005)\tLoss 0.3815 (0.2566)\tPrec 86.719% (90.897%)\n",
      "Epoch: [143][300/391]\tTime 0.050 (0.052)\tData 0.003 (0.004)\tLoss 0.4255 (0.2599)\tPrec 84.375% (90.739%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.408 (0.408)\tLoss 0.4645 (0.4645)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.070% \n",
      "best acc: 85.100000\n",
      "Epoch: [144][0/391]\tTime 0.496 (0.496)\tData 0.422 (0.422)\tLoss 0.2484 (0.2484)\tPrec 91.406% (91.406%)\n",
      "Epoch: [144][100/391]\tTime 0.060 (0.054)\tData 0.003 (0.006)\tLoss 0.2420 (0.2511)\tPrec 90.625% (90.888%)\n",
      "Epoch: [144][200/391]\tTime 0.062 (0.051)\tData 0.003 (0.004)\tLoss 0.2239 (0.2583)\tPrec 93.750% (90.726%)\n",
      "Epoch: [144][300/391]\tTime 0.047 (0.052)\tData 0.002 (0.004)\tLoss 0.2716 (0.2607)\tPrec 89.844% (90.752%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.598 (0.598)\tLoss 0.5023 (0.5023)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.530% \n",
      "best acc: 85.100000\n",
      "Epoch: [145][0/391]\tTime 0.734 (0.734)\tData 0.672 (0.672)\tLoss 0.2208 (0.2208)\tPrec 94.531% (94.531%)\n",
      "Epoch: [145][100/391]\tTime 0.044 (0.054)\tData 0.001 (0.009)\tLoss 0.3317 (0.2548)\tPrec 92.969% (91.344%)\n",
      "Epoch: [145][200/391]\tTime 0.052 (0.051)\tData 0.002 (0.005)\tLoss 0.2633 (0.2597)\tPrec 89.844% (91.010%)\n",
      "Epoch: [145][300/391]\tTime 0.054 (0.051)\tData 0.003 (0.004)\tLoss 0.1392 (0.2599)\tPrec 96.094% (90.939%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.5366 (0.5366)\tPrec 83.594% (83.594%)\n",
      " * Prec 85.110% \n",
      "best acc: 85.110000\n",
      "Epoch: [146][0/391]\tTime 0.546 (0.546)\tData 0.472 (0.472)\tLoss 0.2394 (0.2394)\tPrec 92.188% (92.188%)\n",
      "Epoch: [146][100/391]\tTime 0.052 (0.056)\tData 0.003 (0.007)\tLoss 0.3523 (0.2530)\tPrec 85.938% (91.182%)\n",
      "Epoch: [146][200/391]\tTime 0.051 (0.055)\tData 0.002 (0.005)\tLoss 0.1587 (0.2549)\tPrec 94.531% (91.080%)\n",
      "Epoch: [146][300/391]\tTime 0.041 (0.053)\tData 0.002 (0.004)\tLoss 0.3083 (0.2605)\tPrec 90.625% (90.895%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.421 (0.421)\tLoss 0.5607 (0.5607)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.720% \n",
      "best acc: 85.110000\n",
      "Epoch: [147][0/391]\tTime 0.839 (0.839)\tData 0.768 (0.768)\tLoss 0.3021 (0.3021)\tPrec 89.062% (89.062%)\n",
      "Epoch: [147][100/391]\tTime 0.061 (0.061)\tData 0.003 (0.010)\tLoss 0.2889 (0.2516)\tPrec 92.188% (90.934%)\n",
      "Epoch: [147][200/391]\tTime 0.062 (0.057)\tData 0.003 (0.006)\tLoss 0.2831 (0.2524)\tPrec 89.062% (90.967%)\n",
      "Epoch: [147][300/391]\tTime 0.058 (0.056)\tData 0.003 (0.005)\tLoss 0.1845 (0.2522)\tPrec 90.625% (91.043%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 0.4553 (0.4553)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.450% \n",
      "best acc: 85.110000\n",
      "Epoch: [148][0/391]\tTime 0.564 (0.564)\tData 0.504 (0.504)\tLoss 0.2075 (0.2075)\tPrec 91.406% (91.406%)\n",
      "Epoch: [148][100/391]\tTime 0.055 (0.054)\tData 0.002 (0.007)\tLoss 0.3298 (0.2506)\tPrec 89.844% (91.298%)\n",
      "Epoch: [148][200/391]\tTime 0.049 (0.052)\tData 0.002 (0.005)\tLoss 0.3108 (0.2501)\tPrec 88.281% (91.235%)\n",
      "Epoch: [148][300/391]\tTime 0.050 (0.051)\tData 0.003 (0.004)\tLoss 0.2120 (0.2512)\tPrec 92.188% (91.113%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.330 (0.330)\tLoss 0.5282 (0.5282)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.630% \n",
      "best acc: 85.110000\n",
      "Epoch: [149][0/391]\tTime 0.521 (0.521)\tData 0.454 (0.454)\tLoss 0.2459 (0.2459)\tPrec 92.188% (92.188%)\n",
      "Epoch: [149][100/391]\tTime 0.049 (0.054)\tData 0.002 (0.007)\tLoss 0.2245 (0.2637)\tPrec 92.188% (90.903%)\n",
      "Epoch: [149][200/391]\tTime 0.055 (0.052)\tData 0.003 (0.005)\tLoss 0.1959 (0.2571)\tPrec 92.188% (91.025%)\n",
      "Epoch: [149][300/391]\tTime 0.048 (0.052)\tData 0.002 (0.004)\tLoss 0.3681 (0.2540)\tPrec 90.625% (91.103%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.394 (0.394)\tLoss 0.4961 (0.4961)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.330% \n",
      "best acc: 85.110000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-7\n",
    "epochs = 150\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "untrained = False\n",
    "\n",
    "if (untrained):\n",
    "    for epoch in range(0, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "    \n",
    "        train(trainloader, model, criterion, optimizer, epoch)\n",
    "        \n",
    "        # evaluate on test set\n",
    "        print(\"Validation starts\")\n",
    "        prec = validate(testloader, model, criterion)\n",
    "    \n",
    "        # remember best precision and save checkpoint\n",
    "        is_best = prec > best_prec\n",
    "        best_prec = max(prec,best_prec)\n",
    "        print('best acc: {:1f}'.format(best_prec))\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec': best_prec,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8511/10000 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/final_ResNet20_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send an input and grap the value by using prehook like HW3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "1st convolution's input size: torch.Size([128, 3, 32, 32])\n",
      "2nd convolution's input size: torch.Size([128, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "\n",
    "print(\"1st convolution's input size:\", save_output.outputs[0][0].size())\n",
    "print(\"2nd convolution's input size:\", save_output.outputs[1][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abbfe575-83b0-46a2-a227-09fb38ade009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: torch.Size([128, 3, 32, 32])\n",
      "1: torch.Size([128, 16, 32, 32])\n",
      "2: torch.Size([128, 16, 32, 32])\n",
      "3: torch.Size([128, 16, 32, 32])\n",
      "4: torch.Size([128, 16, 32, 32])\n",
      "5: torch.Size([128, 16, 32, 32])\n",
      "6: torch.Size([128, 16, 32, 32])\n",
      "7: torch.Size([128, 16, 32, 32])\n",
      "8: torch.Size([128, 8, 16, 16])\n",
      "9: torch.Size([128, 16, 32, 32])\n",
      "10: torch.Size([128, 8, 16, 16])\n",
      "11: torch.Size([128, 8, 16, 16])\n",
      "12: torch.Size([128, 8, 16, 16])\n",
      "13: torch.Size([128, 8, 16, 16])\n",
      "14: torch.Size([128, 8, 16, 16])\n",
      "15: torch.Size([128, 64, 8, 8])\n",
      "16: torch.Size([128, 8, 16, 16])\n",
      "17: torch.Size([128, 64, 8, 8])\n",
      "18: torch.Size([128, 64, 8, 8])\n",
      "19: torch.Size([128, 64, 8, 8])\n",
      "20: torch.Size([128, 64, 8, 8])\n",
      "21: torch.Size([128, 64, 8, 8])\n",
      "22: torch.Size([128, 8, 4, 4])\n",
      "23: torch.Size([128, 64, 8, 8])\n",
      "24: torch.Size([128, 8, 4, 4])\n",
      "25: torch.Size([128, 8, 4, 4])\n",
      "26: torch.Size([128, 8, 4, 4])\n",
      "27: torch.Size([128, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(save_output.outputs)):\n",
    "    print(str(index) + \": \" + str(save_output.outputs[index][0].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b340c2e8-bef8-460a-a2d2-0b2024f7de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.layer2[1].conv1.weight_q # quantized value is stored during the training\n",
    "w_alpha = model.layer2[1].conv1.weight_quant.wgt_alpha  # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha / (2**(w_bit-1)-1)    # delta can be calculated by using alpha and w_bit\n",
    "weight_int = weight_q / w_delta # w_int can be calculated by weight_q and w_delta\n",
    "#print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2856498d-ee72-4845-a6f6-c897016dcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bit = 4    \n",
    "x = save_output.outputs[10][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.layer2[1].conv1.act_alpha\n",
    "x_delta = x_alpha / (2**(x_bit)-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q / x_delta\n",
    "#print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee383a5e-6d25-4b1d-aee9-8384c882db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "output_int =  conv_int.forward(x_int)    # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = output_int * x_delta * w_delta  # recover with x_delta and w_delta\n",
    "\n",
    "relu = torch.nn.LeakyReLU(0.5)\n",
    "relu_output_recovered = torch.floor(relu(output_recovered))\n",
    "#print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a59e9c3-a95b-4586-bee3-c9021713d534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 8, 16, 16])\n",
      "torch.Size([128, 8, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "next_input = save_output.outputs[11][0]\n",
    "\n",
    "print(next_input.size())\n",
    "print(relu_output_recovered.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f273e6-12ae-44e4-8401-7f95dc408a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2898, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "difference = abs( next_input - relu_output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb8472ca-12ee-4a0c-a201-dfaacbe873c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(x_int[0,:,:,:].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb6ebee-1879-4180-892c-9f3043afbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = x_int[0,:,:,:]  # pick only one input out of batch\n",
    "# a_int.size() = [64, 32, 32]\n",
    "\n",
    "# conv_int.weight.size() = torch.Size([64, 64, 3, 3])  <- output_ch, input_ch, ki, kj\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([64, 64, 9])\n",
    "                      \n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel \n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(len(icg)/array_size))\n",
    "oc_tileg = range(int(len(ocg)/array_size))\n",
    "\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]\n",
    "\n",
    "\n",
    "a_tile = torch.zeros(len(ic_tileg), array_size,    a_pad.size(1)).cuda() \n",
    "w_tile = torch.zeros(len(oc_tileg)*len(ic_tileg), array_size, array_size, len(kijg)).cuda() \n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    a_tile[ic_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        w_tile[oc_tile*len(oc_tileg) + ic_tile,:,:,:] = w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, :]\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for kij in kijg:\n",
    "    for ic_tile in ic_tileg:       # Tiling into array_sizeXarray_size array\n",
    "        for oc_tile in oc_tileg:   # Tiling into array_sizeXarray_size array        \n",
    "            for nij in p_nijg:       # time domain, sequentially given input\n",
    "                    m = nn.Linear(array_size, array_size, bias=False)\n",
    "                    #m.weight = torch.nn.Parameter(w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, kij])\n",
    "                    m.weight = torch.nn.Parameter(w_tile[len(oc_tileg)*oc_tile+ic_tile,:,:,kij])\n",
    "                    psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile,:,nij]).cuda()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd577acd-420e-48ac-89ea-6dcf6312c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 324])\n",
      "torch.Size([1, 1, 8, 324, 9])\n"
     ]
    }
   ],
   "source": [
    "print(a_tile.size())\n",
    "print(psum.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a54a6afb-0927-43fd-a5b5-cdfcfe6e6fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32\n",
    "\n",
    "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1)\n",
    "o_nijg = range(o_ni_dim**2)    \n",
    "\n",
    "print(len(o_nijg))\n",
    "\n",
    "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
    "  \n",
    "   \n",
    "### SFP accumulation ###\n",
    "for o_nij in o_nijg: \n",
    "    for kij in kijg:\n",
    "        for ic_tile in ic_tileg:    \n",
    "            for oc_tile in oc_tileg:   \n",
    "                out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] = out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] + \\\n",
    "                psum[ic_tile, oc_tile, :, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
    "                ## 4th index = (int(o_nij/30)*32 + o_nij%30) + (int(kij/3)*32 + kij%3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "761db8ee-ed6f-4efc-9340-aed515a8524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "18\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(o_ni_dim)\n",
    "print(a_pad_ni_dim)\n",
    "print(ki_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4d3777-6e26-4d4f-bb7b-9db6e86c084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(-1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(-0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(2., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(-1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp_acc = 0\n",
    "for row in range(8):\n",
    "    for col in range(1):\n",
    "        print(a_tile[0,row,7])\n",
    "        print(w_tile[0,col,row,0])\n",
    "        temp_acc = temp_acc + a_tile[0,row,7] * w_tile[0, col, row, 0]\n",
    "print(temp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488cc825-58f9-42a1-b87a-0a82eb139a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "tile_id = 0 \n",
    "nij = 200 # just a random number\n",
    "X = a_tile[tile_id,:,:]  # [tile_num, array row num, time_steps]\n",
    "\n",
    "if not os.path.exists(\"PLReLU_Files\"):\n",
    "    os.makedirs(\"PLReLU_Files\")\n",
    "\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('PLReLU_Files/activation.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(round(X[7-j,i].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c18ac114-f744-447f-8f0e-f44296db0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "tile_id = 0 \n",
    "kij = 0\n",
    "\n",
    "\n",
    "bit_precision = 4\n",
    "for kij in range(9):\n",
    "    W = w_tile[tile_id,:,:,kij]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "    file = open('PLReLU_Files/weight_' + str(kij) + '.txt', 'w') #write to file\n",
    "    file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "    file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    \n",
    "    for i in range(W.size(1)):  # col\n",
    "        for j in range(W.size(0)): # row #\n",
    "            W_bin = '{0:04b}'.format(round(W[i,7-j].item()) + (16 if (round(W[i,7-j].item()) < 0) else 0))\n",
    "            for k in range(bit_precision):\n",
    "                file.write(W_bin[k])        \n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "    file.close() #close file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dfa2352-d11d-4537-83a1-d03b37dd1302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0 \n",
    "oc_tile_id = 0 \n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 200\n",
    "# psum[len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)]\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "\n",
    "for kij in range(9):\n",
    "    psum_tile = psum[ic_tile_id,oc_tile_id,:,:,kij]  \n",
    "    file = open('PLReLU_Files/psum_' + str(kij) + '.txt', 'w') #write to file\n",
    "    file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "    file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "\n",
    "    for i in range(psum_tile.size(1)):  # time step\n",
    "        for j in range(psum_tile.size(0)): # col #\n",
    "            #psum_bin = '{0:016b}'.format(round(psum_tile[psum_tile.size(0)-1-j,i].item()) + (2**bit_precision if (round(psum_tile[psum_tile.size(0)-1-j,i].item()) < 0) else 0))\n",
    "            curr_psum = round(psum_tile[psum_tile.size(0)-1-j,i].item())\n",
    "            if (i == 7 and kij == 0):\n",
    "                print(curr_psum)\n",
    "            if (curr_psum < 0):\n",
    "                if (i == 7 and kij == 0):\n",
    "                    print('{0:016b}'.format(curr_psum))\n",
    "                curr_psum = curr_psum + 2**bit_precision\n",
    "                if (i == 7 and kij == 0):\n",
    "                    print('{0:016b}'.format(curr_psum))\n",
    "            psum_bin = '{0:016b}'.format(curr_psum)\n",
    "            \n",
    "            for k in range(bit_precision):\n",
    "                file.write(psum_bin[k])        \n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "    file.close() #close file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49a77564-a1b3-4abc-8eda-f725245e8271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111010010\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111110110001\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111110101110\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111110010001\n",
      "1111111111001000\n",
      "1111111111001000\n",
      "1111111110110001\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111110011011\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111110010111\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110011010\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110100010\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111110111110\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111110100001\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111110011000\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111100101\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111110010000\n",
      "1111111111001000\n",
      "1111111111001000\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111110100110\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110110010\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111110010110\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111110010011\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111101011011\n",
      "1111111110101101\n",
      "1111111110101101\n",
      "1111111101011011\n",
      "1111111110101101\n",
      "1111111110101101\n",
      "1111111110011000\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110001111\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111110011000\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111110000111\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111100010\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111101111101\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111110011000\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110011010\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111110100101\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110101101\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111000001\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111110111111\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111010010\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111110111111\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111110100001\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111110110001\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111111001100\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111110011001\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111101111101\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111110011100\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110101110\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111110011000\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110011111\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111001010\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111110000011\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111110011010\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111111001011\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111101001\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111110100000\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111110111111\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110101110\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110010111\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111110110010\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111110000001\n",
      "1111111111000000\n",
      "1111111111000000\n",
      "1111111110110001\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110010100\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111100111111\n",
      "1111111110011111\n",
      "1111111110011111\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111001100\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111110000000\n",
      "1111111111000000\n",
      "1111111111000000\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110010101\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111110000100\n",
      "1111111111000010\n",
      "1111111111000010\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111110101010\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111100101\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111110000011\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110001110\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111101011000\n",
      "1111111110101100\n",
      "1111111110101100\n",
      "1111111110011110\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111101101101\n",
      "1111111110110110\n",
      "1111111110110110\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111010000\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110000010\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111110011100\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110101110\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110000011\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111000001\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111101011110\n",
      "1111111110101111\n",
      "1111111110101111\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110110000\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111101110000\n",
      "1111111110111000\n",
      "1111111110111000\n",
      "1111111101111011\n",
      "1111111110111101\n",
      "1111111110111101\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110011101\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111101010111\n",
      "1111111110101011\n",
      "1111111110101011\n",
      "1111111110011100\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111110101101\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111011010011\n",
      "1111111101101001\n",
      "1111111101101001\n",
      "1111111110000010\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111110101101\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111110001001\n",
      "1111111111000100\n",
      "1111111111000100\n",
      "1111111110000101\n",
      "1111111111000010\n",
      "1111111111000010\n",
      "1111111111010010\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111001100\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110011011\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110111111\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111001000\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111101010111\n",
      "1111111110101011\n",
      "1111111110101011\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110000110\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111101010101\n",
      "1111111110101010\n",
      "1111111110101010\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110101010\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111110100011\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110100111\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111110001110\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111110100100\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111100010111\n",
      "1111111110001011\n",
      "1111111110001011\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111101011010\n",
      "1111111110101101\n",
      "1111111110101101\n",
      "1111111110011100\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111110100010\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111110100001\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111101111100\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111110100101\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111110010101\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111101001\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111110010110\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111100010\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111101100010\n",
      "1111111110110001\n",
      "1111111110110001\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110010100\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111110000010\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111101100010\n",
      "1111111110110001\n",
      "1111111110110001\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111110100110\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111110001101\n",
      "1111111111000110\n",
      "1111111111000110\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111101001\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110110000\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111001010\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111101110010\n",
      "1111111110111001\n",
      "1111111110111001\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110000111\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111110111001\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111101101101\n",
      "1111111110110110\n",
      "1111111110110110\n",
      "1111111101100000\n",
      "1111111110110000\n",
      "1111111110110000\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111110101010\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110101110\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111110101101\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111101110000\n",
      "1111111110111000\n",
      "1111111110111000\n",
      "1111111110001000\n",
      "1111111111000100\n",
      "1111111111000100\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111101001101\n",
      "1111111110100110\n",
      "1111111110100110\n",
      "1111111101010001\n",
      "1111111110101000\n",
      "1111111110101000\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111110100010\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111101110111\n",
      "1111111110111011\n",
      "1111111110111011\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111110100101\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111101001011\n",
      "1111111110100101\n",
      "1111111110100101\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110011010\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111101111001\n",
      "1111111110111100\n",
      "1111111110111100\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111110010001\n",
      "1111111111001000\n",
      "1111111111001000\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110001001\n",
      "1111111111000100\n",
      "1111111111000100\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111101110111\n",
      "1111111110111011\n",
      "1111111110111011\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111110100111\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111110011101\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111110001110\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110101101\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111101111110\n",
      "1111111110111111\n",
      "1111111110111111\n",
      "1111111110001010\n",
      "1111111111000101\n",
      "1111111111000101\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111110110100\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110100011\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111101011111\n",
      "1111111110101111\n",
      "1111111110101111\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110100010\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111110000110\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111110011010\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111110111001\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111101111101\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111110000000\n",
      "1111111111000000\n",
      "1111111111000000\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111000001\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111110001010\n",
      "1111111111000101\n",
      "1111111111000101\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111001000\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111110011110\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110100000\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111101000111\n",
      "1111111110100011\n",
      "1111111110100011\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110000010\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111101010001\n",
      "1111111110101000\n",
      "1111111110101000\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111100001010\n",
      "1111111110000101\n",
      "1111111110000101\n",
      "1111111100100111\n",
      "1111111110010011\n",
      "1111111110010011\n",
      "1111111110010110\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111101011111\n",
      "1111111110101111\n",
      "1111111110101111\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111110000100\n",
      "1111111111000010\n",
      "1111111111000010\n",
      "1111111100100110\n",
      "1111111110010011\n",
      "1111111110010011\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111101010111\n",
      "1111111110101011\n",
      "1111111110101011\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111101000101\n",
      "1111111110100010\n",
      "1111111110100010\n",
      "1111111111010000\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111110010000\n",
      "1111111111001000\n",
      "1111111111001000\n",
      "1111111101110100\n",
      "1111111110111010\n",
      "1111111110111010\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111110100001\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110011001\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111010000\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111101111010\n",
      "1111111110111101\n",
      "1111111110111101\n",
      "1111111110000001\n",
      "1111111111000000\n",
      "1111111111000000\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111110111001\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110111001\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111110001110\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110011011\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111101011101\n",
      "1111111110101110\n",
      "1111111110101110\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111100011111\n",
      "1111111110001111\n",
      "1111111110001111\n",
      "1111111101111101\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111100010000\n",
      "1111111110001000\n",
      "1111111110001000\n",
      "1111111101111111\n",
      "1111111110111111\n",
      "1111111110111111\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111101111011\n",
      "1111111110111101\n",
      "1111111110111101\n",
      "1111111100110100\n",
      "1111111110011010\n",
      "1111111110011010\n",
      "1111111110001000\n",
      "1111111111000100\n",
      "1111111111000100\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111110100111\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111110001011\n",
      "1111111111000101\n",
      "1111111111000101\n",
      "1111111101001000\n",
      "1111111110100100\n",
      "1111111110100100\n",
      "1111111101101111\n",
      "1111111110110111\n",
      "1111111110110111\n",
      "1111111110110100\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111110100010\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111111010000\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110000110\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111110000111\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111111010000\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111111001011\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111101110001\n",
      "1111111110111000\n",
      "1111111110111000\n",
      "1111111110110001\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111100011011\n",
      "1111111110001101\n",
      "1111111110001101\n",
      "1111111110001111\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111101011001\n",
      "1111111110101100\n",
      "1111111110101100\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111100100100\n",
      "1111111110010010\n",
      "1111111110010010\n",
      "1111111101011000\n",
      "1111111110101100\n",
      "1111111110101100\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111101101111\n",
      "1111111110110111\n",
      "1111111110110111\n",
      "1111111110100000\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111101101101\n",
      "1111111110110110\n",
      "1111111110110110\n",
      "1111111101110000\n",
      "1111111110111000\n",
      "1111111110111000\n",
      "1111111100101110\n",
      "1111111110010111\n",
      "1111111110010111\n",
      "1111111110001111\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111101111101\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111101010100\n",
      "1111111110101010\n",
      "1111111110101010\n",
      "1111111110000010\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111101111101\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111110010111\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111110100011\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111110100101\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111100001100\n",
      "1111111110000110\n",
      "1111111110000110\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111101100011\n",
      "1111111110110001\n",
      "1111111110110001\n",
      "1111111110101101\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111100101101\n",
      "1111111110010110\n",
      "1111111110010110\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111101011101\n",
      "1111111110101110\n",
      "1111111110101110\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111001010\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111101010010\n",
      "1111111110101001\n",
      "1111111110101001\n",
      "1111111110101110\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111101101011\n",
      "1111111110110101\n",
      "1111111110110101\n",
      "1111111110011110\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111100101000\n",
      "1111111110010100\n",
      "1111111110010100\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111101001\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111100010\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111101011001\n",
      "1111111110101100\n",
      "1111111110101100\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110100000\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111101100010\n",
      "1111111110110001\n",
      "1111111110110001\n",
      "1111111110100011\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111110000100\n",
      "1111111111000010\n",
      "1111111111000010\n",
      "1111111110011100\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111100101\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111110000110\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111110001110\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111111101001\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111110110000\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111101001100\n",
      "1111111110100110\n",
      "1111111110100110\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111110001001\n",
      "1111111111000100\n",
      "1111111111000100\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111101111110\n",
      "1111111110111111\n",
      "1111111110111111\n",
      "1111111110010111\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111101001110\n",
      "1111111110100111\n",
      "1111111110100111\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111101000000\n",
      "1111111110100000\n",
      "1111111110100000\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111100111010\n",
      "1111111110011101\n",
      "1111111110011101\n",
      "1111111101100100\n",
      "1111111110110010\n",
      "1111111110110010\n",
      "1111111100010110\n",
      "1111111110001011\n",
      "1111111110001011\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111100001100\n",
      "1111111110000110\n",
      "1111111110000110\n",
      "1111111110000011\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111101111111\n",
      "1111111110111111\n",
      "1111111110111111\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110000100\n",
      "1111111111000010\n",
      "1111111111000010\n",
      "1111111110001111\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111110001110\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111110111111\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111100100101\n",
      "1111111110010010\n",
      "1111111110010010\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111100110000\n",
      "1111111110011000\n",
      "1111111110011000\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111100001110\n",
      "1111111110000111\n",
      "1111111110000111\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111101001001\n",
      "1111111110100100\n",
      "1111111110100100\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111100100101\n",
      "1111111110010010\n",
      "1111111110010010\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111001100\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111101010111\n",
      "1111111110101011\n",
      "1111111110101011\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110000010\n",
      "1111111111000001\n",
      "1111111111000001\n",
      "1111111111001010\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111110101000\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111110101000\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111110111110\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111110111111\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111001100\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111110101011\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111111000001\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111000001\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111101101011\n",
      "1111111110110101\n",
      "1111111110110101\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111101011011\n",
      "1111111110101101\n",
      "1111111110101101\n",
      "1111111110100101\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111110100010\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111110010000\n",
      "1111111111001000\n",
      "1111111111001000\n",
      "1111111101011101\n",
      "1111111110101110\n",
      "1111111110101110\n",
      "1111111110001011\n",
      "1111111111000101\n",
      "1111111111000101\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110100011\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111101011010\n",
      "1111111110101101\n",
      "1111111110101101\n",
      "1111111111100101\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111101011110\n",
      "1111111110101111\n",
      "1111111110101111\n",
      "1111111101011101\n",
      "1111111110101110\n",
      "1111111110101110\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111101011111\n",
      "1111111110101111\n",
      "1111111110101111\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111100110000\n",
      "1111111110011000\n",
      "1111111110011000\n",
      "1111111100111100\n",
      "1111111110011110\n",
      "1111111110011110\n",
      "1111111101111011\n",
      "1111111110111101\n",
      "1111111110111101\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110011111\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111110010011\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110010011\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111101010011\n",
      "1111111110101001\n",
      "1111111110101001\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111101011110\n",
      "1111111110101111\n",
      "1111111110101111\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111100111010\n",
      "1111111110011101\n",
      "1111111110011101\n",
      "1111111101111100\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111011000001\n",
      "1111111101100000\n",
      "1111111101100000\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111101100010\n",
      "1111111110110001\n",
      "1111111110110001\n",
      "1111111100111000\n",
      "1111111110011100\n",
      "1111111110011100\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111100110001\n",
      "1111111110011000\n",
      "1111111110011000\n",
      "1111111110010100\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111101011010\n",
      "1111111110101101\n",
      "1111111110101101\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111101111100\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111110010110\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111110011001\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111110011110\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111110111110\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110011011\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110100001\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111110110000\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111110011000\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111101010011\n",
      "1111111110101001\n",
      "1111111110101001\n",
      "1111111110110000\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111100111011\n",
      "1111111110011101\n",
      "1111111110011101\n",
      "1111111101110011\n",
      "1111111110111001\n",
      "1111111110111001\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111101100011\n",
      "1111111110110001\n",
      "1111111110110001\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111101000101\n",
      "1111111110100010\n",
      "1111111110100010\n",
      "1111111101010101\n",
      "1111111110101010\n",
      "1111111110101010\n",
      "1111111110100011\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111110011111\n",
      "1111111111001111\n",
      "1111111111001111\n",
      "1111111101100001\n",
      "1111111110110000\n",
      "1111111110110000\n",
      "1111111100100001\n",
      "1111111110010000\n",
      "1111111110010000\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111101111101\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111101011010\n",
      "1111111110101101\n",
      "1111111110101101\n",
      "1111111101001110\n",
      "1111111110100111\n",
      "1111111110100111\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111101100100\n",
      "1111111110110010\n",
      "1111111110110010\n",
      "1111111101000111\n",
      "1111111110100011\n",
      "1111111110100011\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110011001\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111110100000\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111110011000\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110100101\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111111100010\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111110001001\n",
      "1111111111000100\n",
      "1111111111000100\n",
      "1111111110100011\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110010101\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111110010010\n",
      "1111111111001001\n",
      "1111111111001001\n",
      "1111111110001111\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111101001\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111101101100\n",
      "1111111110110110\n",
      "1111111110110110\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111101010001\n",
      "1111111110101000\n",
      "1111111110101000\n",
      "1111111100111111\n",
      "1111111110011111\n",
      "1111111110011111\n",
      "1111111101101100\n",
      "1111111110110110\n",
      "1111111110110110\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111110001101\n",
      "1111111111000110\n",
      "1111111111000110\n",
      "1111111100101010\n",
      "1111111110010101\n",
      "1111111110010101\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111110110001\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111110011101\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111110010000\n",
      "1111111111001000\n",
      "1111111111001000\n",
      "1111111100110000\n",
      "1111111110011000\n",
      "1111111110011000\n",
      "1111111110000111\n",
      "1111111111000011\n",
      "1111111111000011\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111101111010\n",
      "1111111110111101\n",
      "1111111110111101\n",
      "1111111101101010\n",
      "1111111110110101\n",
      "1111111110110101\n",
      "1111111100100000\n",
      "1111111110010000\n",
      "1111111110010000\n",
      "1111111101010001\n",
      "1111111110101000\n",
      "1111111110101000\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111101000001\n",
      "1111111110100000\n",
      "1111111110100000\n",
      "1111111101100011\n",
      "1111111110110001\n",
      "1111111110110001\n",
      "1111111101001011\n",
      "1111111110100101\n",
      "1111111110100101\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111101010011\n",
      "1111111110101001\n",
      "1111111110101001\n",
      "1111111101100000\n",
      "1111111110110000\n",
      "1111111110110000\n",
      "1111111110001010\n",
      "1111111111000101\n",
      "1111111111000101\n",
      "1111111110100100\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111101101100\n",
      "1111111110110110\n",
      "1111111110110110\n",
      "1111111101111100\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111000001\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111110011001\n",
      "1111111111001100\n",
      "1111111111001100\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111101111110\n",
      "1111111110111111\n",
      "1111111110111111\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111110011011\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111110100111\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111111111000\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111110100001\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111110110010\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111110101000\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111101111001\n",
      "1111111110111100\n",
      "1111111110111100\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111111110\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111100101\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111100011\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111110110\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111110101110\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111110000001\n",
      "1111111111000000\n",
      "1111111111000000\n",
      "1111111110101010\n",
      "1111111111010101\n",
      "1111111111010101\n",
      "1111111110101000\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111101111100\n",
      "1111111110111110\n",
      "1111111110111110\n",
      "1111111110100111\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111110001111\n",
      "1111111111000111\n",
      "1111111111000111\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111110011100\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111110010101\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111110010101\n",
      "1111111111001010\n",
      "1111111111001010\n",
      "1111111110100000\n",
      "1111111111010000\n",
      "1111111111010000\n",
      "1111111110100101\n",
      "1111111111010010\n",
      "1111111111010010\n",
      "1111111110011010\n",
      "1111111111001101\n",
      "1111111111001101\n",
      "1111111101001010\n",
      "1111111110100101\n",
      "1111111110100101\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111101010001\n",
      "1111111110101000\n",
      "1111111110101000\n",
      "1111111111111010\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111111110010\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111101110111\n",
      "1111111110111011\n",
      "1111111110111011\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111001000\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111110101\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111010000\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110100010\n",
      "1111111111010001\n",
      "1111111111010001\n",
      "1111111111001100\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111101101\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111100111\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111001010\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111111101\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111001101\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111010010\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111011101\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111111100001\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111110110001\n",
      "1111111111011000\n",
      "1111111111011000\n",
      "1111111110111100\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111010001\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111101110100\n",
      "1111111110111010\n",
      "1111111110111010\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111111100000\n",
      "1111111111110000\n",
      "1111111111110000\n",
      "1111111110010111\n",
      "1111111111001011\n",
      "1111111111001011\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111001100\n",
      "1111111111100110\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111111100\n",
      "1111111111111110\n",
      "1111111111111110\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111010011\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111100010\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111101010\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111001011\n",
      "1111111111100101\n",
      "1111111111100101\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111111011\n",
      "1111111111111101\n",
      "1111111111111101\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111111111\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111111101011\n",
      "1111111111110101\n",
      "1111111111110101\n",
      "1111111110110101\n",
      "1111111111011010\n",
      "1111111111011010\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111111001\n",
      "1111111111111100\n",
      "1111111111111100\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111101111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111111011\n",
      "1111111111111011\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111011111\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111111100100\n",
      "1111111111110010\n",
      "1111111111110010\n",
      "1111111110101001\n",
      "1111111111010100\n",
      "1111111111010100\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111100110\n",
      "1111111111110011\n",
      "1111111111110011\n",
      "1111111110111110\n",
      "1111111111011111\n",
      "1111111111011111\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111011110\n",
      "1111111111101111\n",
      "1111111111101111\n",
      "1111111110101100\n",
      "1111111111010110\n",
      "1111111111010110\n",
      "1111111111110001\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111101110100\n",
      "1111111110111010\n",
      "1111111110111010\n",
      "1111111111110000\n",
      "1111111111111000\n",
      "1111111111111000\n",
      "1111111110100111\n",
      "1111111111010011\n",
      "1111111111010011\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111110011101\n",
      "1111111111001110\n",
      "1111111111001110\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110110110\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111000000\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111000101\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111110111001\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111001111\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111100010\n",
      "1111111111110001\n",
      "1111111111110001\n",
      "1111111110111101\n",
      "1111111111011110\n",
      "1111111111011110\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111111001000\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111000011\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111101000\n",
      "1111111111110100\n",
      "1111111111110100\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011001\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111000001\n",
      "1111111111100000\n",
      "1111111111100000\n",
      "1111111111101110\n",
      "1111111111110111\n",
      "1111111111110111\n",
      "1111111111000010\n",
      "1111111111100001\n",
      "1111111111100001\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111000111\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011010\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111110110111\n",
      "1111111111011011\n",
      "1111111111011011\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111010100\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111110111011\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111010010\n",
      "1111111111101001\n",
      "1111111111101001\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111011100\n",
      "1111111111101110\n",
      "1111111111101110\n",
      "1111111110111010\n",
      "1111111111011101\n",
      "1111111111011101\n",
      "1111111111001110\n",
      "1111111111100111\n",
      "1111111111100111\n",
      "1111111111001001\n",
      "1111111111100100\n",
      "1111111111100100\n",
      "1111111111011000\n",
      "1111111111101100\n",
      "1111111111101100\n",
      "1111111111000100\n",
      "1111111111100010\n",
      "1111111111100010\n",
      "1111111111010101\n",
      "1111111111101010\n",
      "1111111111101010\n",
      "1111111111000110\n",
      "1111111111100011\n",
      "1111111111100011\n",
      "1111111111010111\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111111010000\n",
      "1111111111101000\n",
      "1111111111101000\n",
      "1111111110111000\n",
      "1111111111011100\n",
      "1111111111011100\n",
      "1111111111011011\n",
      "1111111111101101\n",
      "1111111111101101\n",
      "1111111111101100\n",
      "1111111111110110\n",
      "1111111111110110\n",
      "1111111110110011\n",
      "1111111111011001\n",
      "1111111111011001\n",
      "1111111111110011\n",
      "1111111111111001\n",
      "1111111111111001\n",
      "1111111111010110\n",
      "1111111111101011\n",
      "1111111111101011\n",
      "1111111110101111\n",
      "1111111111010111\n",
      "1111111111010111\n",
      "1111111111110100\n",
      "1111111111111010\n",
      "1111111111111010\n"
     ]
    }
   ],
   "source": [
    "### Complete this cell ###\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "# out is array of size columns x len(o_nijg)\n",
    "\n",
    "file = open('PLReLU_Files/out.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(out.size(1)):  # time step\n",
    "    for j in range(out.size(0)): # row #\n",
    "        output = round(out[out.size(0)-1-j,i].item())\n",
    "        shifted = math.floor(output * 0.5)\n",
    "        complement = shifted + 2**bit_precision\n",
    "        #if (output < 0):\n",
    "            #print('{0:016b}'.format(output + 2**bit_precision))\n",
    "            #print('{0:016b}'.format(shifted + 2**bit_precision))\n",
    "            #print('{0:016b}'.format(complement))\n",
    "        out_bin = '{0:016b}'.format(complement if (round(out[out.size(0)-1-j,i].item()) < 0) else (round(out[out.size(0)-1-j,i].item())))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(out_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b98243-6d02-47d8-b4cd-2827858306a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 16.0000,  25.0000,  17.0000,  -4.0000,  25.0000,  22.0000,  -3.0000,\n",
      "        -25.0000, -13.0000, -27.0000,  18.0000,  18.0000,   8.0000,  38.0000,\n",
      "         27.0000, -10.0000,  19.0000,  53.0000,  49.0000,  99.0000,  85.0000,\n",
      "         66.0000,  23.0000,   7.0000,  13.0000,  55.0000,  76.0000,  59.0000,\n",
      "         38.0000,  11.0000,  67.0000,  90.0000,  18.0000,  37.0000,  18.0000,\n",
      "         93.0000,  91.0000,  50.0000,  31.0000,  15.0000,  72.0000, 161.0000,\n",
      "        105.0000,  55.0000,  37.0000,  27.0000,  69.0000,  90.0000,  21.0000,\n",
      "         75.0000,  89.0000, 112.0000,  69.0000,  28.0000,  35.0000, 105.0000,\n",
      "         65.0000, 148.0000,  97.0000,  55.0000,  26.0000,  36.0000,  95.0000,\n",
      "         85.0000,   7.0000,  91.0000, 123.0000, 118.0000,  41.0000,   6.0000,\n",
      "         34.0000,  76.0000,  63.0000,  75.0000,  66.0000,  81.0000,  45.0000,\n",
      "         31.0000, 106.0000,  85.0000,  17.0000,  54.0000,  80.0000,  76.0000,\n",
      "         25.0000,  47.0000, 129.0000, 144.0000, 113.0000,  88.0000,  37.0000,\n",
      "         76.0000,  84.0000,  34.0000, 101.0000,  79.0000,  20.0000,  40.0000,\n",
      "         46.0000, 123.0000, 168.0000, 199.0000, 269.0000, 208.0000, 151.0000,\n",
      "        133.0000,  39.0000,  68.0000,  49.0000,  24.0000, 109.0000,  85.0000,\n",
      "         12.0000,  58.0000,  72.0000, 159.0000, 272.0000, 267.0000, 243.0000,\n",
      "        196.0000, 158.0000, 103.0000,  57.0000, 126.0000,  70.0000,  73.0000,\n",
      "        134.0000, 112.0000,   6.0000,  50.0000, 105.0000, 125.0000, 223.0000,\n",
      "        215.0000, 150.0000,  58.0000,  16.0000, -16.0000,  16.0000,  73.0000,\n",
      "         12.0000,  66.0000, 142.0000,  84.0000,  50.0000, 103.0000,  68.0000,\n",
      "        113.0000, 132.0000, 109.0000,  95.0000,  39.0000,  31.0000,  67.0000,\n",
      "         58.0000,  17.0000, -29.0000,  16.0000, 168.0000, 123.0000,  30.0000,\n",
      "         69.0000,  27.0000,  36.0000, 110.0000,  92.0000,  87.0000,  89.0000,\n",
      "         94.0000,  61.0000, 114.0000, 102.0000,  31.0000,  35.0000, 115.0000,\n",
      "        126.0000, -16.0000,  25.0000,  77.0000,  25.0000,  33.0000,  76.0000,\n",
      "         81.0000,  90.0000,  81.0000,  38.0000,  42.0000, 129.0000, 116.0000,\n",
      "         62.0000,  72.0000,  81.0000,  27.0000,  38.0000,  41.0000,  96.0000,\n",
      "         73.0000, 110.0000,  99.0000, 100.0000,  91.0000,  80.0000,  83.0000,\n",
      "        107.0000, 134.0000, 116.0000, 117.0000, 105.0000,  26.0000, 105.0000,\n",
      "         80.0000,  64.0000,  83.0000, 118.0000, 126.0000, 112.0000,  87.0000,\n",
      "         54.0000,  55.0000,  81.0000,  92.0000, 103.0000, 120.0000,  99.0000,\n",
      "         87.0000, 191.0000, 162.0000, 135.0000, 105.0000,  96.0000,  81.0000,\n",
      "         66.0000,  49.0000,  45.0000,  49.0000,  54.0000,  56.0000,  30.0000,\n",
      "         47.0000,  53.0000,  99.0000, 135.0000, 107.0000,  93.0000,  83.0000,\n",
      "         86.0000,  77.0000,  75.0000,  81.0000,  90.0000,  91.0000,  96.0000,\n",
      "        106.0000, 101.0000,  99.0000,  24.0000], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f78b6e98-7fac-45af-94a6-a55dbfaeaec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc0c43be-e3cc-48cd-aea4-3299dfc8c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 324])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d8aca94-aba5-4936-94c8-fc5ed7683b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(w_int.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd2df4c5-b134-499b-a9be-a76ec46936c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 324])\n"
     ]
    }
   ],
   "source": [
    "print(psum_tile.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aa319e2-f035-4eb4-9c9f-d5fc8a7be8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(o_ni_dim)\n",
    "print(a_pad_ni_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b01dbc-115e-41fa-8f84-1baea3d44043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d68f87-a91a-4398-b620-b53c1f040146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
